{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f2aa0d",
   "metadata": {
    "id": "SX7UC-8jTsp7",
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "\n",
    "<center><h1>The Annotated Transformer</h1> </center>\n",
    "\n",
    "\n",
    "<center>\n",
    "<p><a href=\"https://arxiv.org/abs/1706.03762\">Attention is All You Need\n",
    "</a></p>\n",
    "</center>\n",
    "\n",
    "<img src=\"images/aiayn.png\" width=\"70%\"/>\n",
    "\n",
    "* *v2022: Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak,\n",
    "   and Stella Biderman.*\n",
    "* *[Original](https://nlp.seas.harvard.edu/2018/04/03/attention.html):\n",
    "   [Sasha Rush](http://rush-nlp.com/).*\n",
    "\n",
    "\n",
    "The Transformer has been on a lot of\n",
    "people's minds over the last <s>year</s> five years.\n",
    "This post presents an annotated version of the paper in the\n",
    "form of a line-by-line implementation. It reorders and deletes\n",
    "some sections from the original paper and adds comments\n",
    "throughout. This document itself is a working notebook, and should\n",
    "be a completely usable implementation.\n",
    "Code is available\n",
    "[here](https://github.com/harvardnlp/annotated-transformer/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d01c87b",
   "metadata": {
    "id": "RSntDwKhTsp-",
    "lines_to_next_cell": 2
   },
   "source": [
    "<h3> Table of Contents </h3>\n",
    "<ul>\n",
    "<li><a href=\"#prelims\">Prelims</a></li>\n",
    "<li><a href=\"#background\">Background</a></li>\n",
    "<li><a href=\"#part-1-model-architecture\">Part 1: Model Architecture</a></li>\n",
    "<li><a href=\"#model-architecture\">Model Architecture</a><ul>\n",
    "<li><a href=\"#encoder-and-decoder-stacks\">Encoder and Decoder Stacks</a></li>\n",
    "<li><a href=\"#position-wise-feed-forward-networks\">Position-wise Feed-Forward\n",
    "Networks</a></li>\n",
    "<li><a href=\"#embeddings-and-softmax\">Embeddings and Softmax</a></li>\n",
    "<li><a href=\"#positional-encoding\">Positional Encoding</a></li>\n",
    "<li><a href=\"#full-model\">Full Model</a></li>\n",
    "<li><a href=\"#inference\">Inference:</a></li>\n",
    "</ul></li>\n",
    "<li><a href=\"#part-2-model-training\">Part 2: Model Training</a></li>\n",
    "<li><a href=\"#training\">Training</a><ul>\n",
    "<li><a href=\"#batches-and-masking\">Batches and Masking</a></li>\n",
    "<li><a href=\"#training-loop\">Training Loop</a></li>\n",
    "<li><a href=\"#training-data-and-batching\">Training Data and Batching</a></li>\n",
    "<li><a href=\"#hardware-and-schedule\">Hardware and Schedule</a></li>\n",
    "<li><a href=\"#optimizer\">Optimizer</a></li>\n",
    "<li><a href=\"#regularization\">Regularization</a></li>\n",
    "</ul></li>\n",
    "<li><a href=\"#a-first-example\">A First Example</a><ul>\n",
    "<li><a href=\"#synthetic-data\">Synthetic Data</a></li>\n",
    "<li><a href=\"#loss-computation\">Loss Computation</a></li>\n",
    "<li><a href=\"#greedy-decoding\">Greedy Decoding</a></li>\n",
    "</ul></li>\n",
    "<li><a href=\"#part-3-a-real-world-example\">Part 3: A Real World Example</a>\n",
    "<ul>\n",
    "<li><a href=\"#data-loading\">Data Loading</a></li>\n",
    "<li><a href=\"#iterators\">Iterators</a></li>\n",
    "<li><a href=\"#training-the-system\">Training the System</a></li>\n",
    "</ul></li>\n",
    "<li><a href=\"#additional-components-bpe-search-averaging\">Additional\n",
    "Components: BPE, Search, Averaging</a></li>\n",
    "<li><a href=\"#results\">Results</a><ul>\n",
    "<li><a href=\"#attention-visualization\">Attention Visualization</a></li>\n",
    "<li><a href=\"#encoder-self-attention\">Encoder Self Attention</a></li>\n",
    "<li><a href=\"#decoder-self-attention\">Decoder Self Attention</a></li>\n",
    "<li><a href=\"#decoder-src-attention\">Decoder Src Attention</a></li>\n",
    "</ul></li>\n",
    "<li><a href=\"#conclusion\">Conclusion</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405575f7",
   "metadata": {
    "id": "BhmOhn9lTsp8"
   },
   "source": [
    "# Prelims\n",
    "\n",
    "<a href=\"#background\">Skip</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f8b89b",
   "metadata": {
    "id": "NwClcbH6Tsp8"
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b6721",
   "metadata": {
    "id": "NwClcbH6Tsp8",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# # Uncomment for colab\n",
    "# #\n",
    "# !pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil\n",
    "# !python -m spacy download de_core_news_sm\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a8674",
   "metadata": {
    "id": "v1-1MX6oTsp9",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "import spacy\n",
    "import GPUtil\n",
    "import warnings\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "# Set to False to skip notebook execution (e.g. for debugging)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0678eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some convenience helper functions used throughout the notebook\n",
    "\n",
    "\n",
    "def is_interactive_notebook():\n",
    "    return __name__ == \"__main__\"\n",
    "\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "\n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344edc16",
   "metadata": {
    "id": "jx49WRyfTsp-"
   },
   "source": [
    "> My comments are blockquoted. The main text is all from the paper itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6902f8",
   "metadata": {
    "id": "7phVeWghTsp_"
   },
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa45d5fd",
   "metadata": {
    "id": "83ZDS91dTsqA"
   },
   "source": [
    "\n",
    "The goal of reducing sequential computation also forms the\n",
    "foundation of the Extended Neural GPU, ByteNet and ConvS2S, all of\n",
    "which use convolutional neural networks as basic building block,\n",
    "computing hidden representations in parallel for all input and\n",
    "output positions. In these models, the number of operations required\n",
    "to relate signals from two arbitrary input or output positions grows\n",
    "in the distance between positions, linearly for ConvS2S and\n",
    "logarithmically for ByteNet. This makes it more difficult to learn\n",
    "dependencies between distant positions. In the Transformer this is\n",
    "reduced to a constant number of operations, albeit at the cost of\n",
    "reduced effective resolution due to averaging attention-weighted\n",
    "positions, an effect we counteract with Multi-Head Attention.\n",
    "\n",
    "Self-attention, sometimes called intra-attention is an attention\n",
    "mechanism relating different positions of a single sequence in order\n",
    "to compute a representation of the sequence. Self-attention has been\n",
    "used successfully in a variety of tasks including reading\n",
    "comprehension, abstractive summarization, textual entailment and\n",
    "learning task-independent sentence representations. End-to-end\n",
    "memory networks are based on a recurrent attention mechanism instead\n",
    "of sequencealigned recurrence and have been shown to perform well on\n",
    "simple-language question answering and language modeling tasks.\n",
    "\n",
    "To the best of our knowledge, however, the Transformer is the first\n",
    "transduction model relying entirely on self-attention to compute\n",
    "representations of its input and output without using sequence\n",
    "aligned RNNs or convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df0f1b",
   "metadata": {},
   "source": [
    "# Part 1: Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601d2112",
   "metadata": {
    "id": "pFrPajezTsqB"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2243928f",
   "metadata": {
    "id": "ReuU_h-fTsqB"
   },
   "source": [
    "\n",
    "Most competitive neural sequence transduction models have an\n",
    "encoder-decoder structure\n",
    "[(cite)](https://arxiv.org/abs/1409.0473). Here, the encoder maps an\n",
    "input sequence of symbol representations $(x_1, ..., x_n)$ to a\n",
    "sequence of continuous representations $\\mathbf{z} = (z_1, ...,\n",
    "z_n)$. Given $\\mathbf{z}$, the decoder then generates an output\n",
    "sequence $(y_1,...,y_m)$ of symbols one element at a time. At each\n",
    "step the model is auto-regressive\n",
    "[(cite)](https://arxiv.org/abs/1308.0850), consuming the previously\n",
    "generated symbols as additional input when generating the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22179fb",
   "metadata": {
    "id": "k0XGXhzRTsqB"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many\n",
    "    other models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794875c8-a5ec-4a5d-bb8e-c246448c82d9",
   "metadata": {},
   "source": [
    "这段代码定义了一个标准的编码器-解码器（Encoder-Decoder）架构，作为其他模型的基础。\n",
    "\n",
    "在初始化函数中，`EncoderDecoder` 类接受五个参数：\n",
    "- `encoder`：编码器模型。\n",
    "- `decoder`：解码器模型。\n",
    "- `src_embed`：源语言嵌入层。\n",
    "- `tgt_embed`：目标语言嵌入层。\n",
    "- `generator`：生成器（输出层）。\n",
    "\n",
    "在初始化过程中，通过调用 `super(EncoderDecoder, self).__init__()` 构造父类的初始化函数。\n",
    "\n",
    "在前向传播函数中，输入包括源语言输入序列 `src`、目标语言输入序列 `tgt`，以及对应的掩码 `src_mask` 和 `tgt_mask`。\n",
    "\n",
    "- `encode` 方法：通过调用源语言嵌入层 `self.src_embed(src)` 将源语言输入序列 `src` 进行嵌入操作，然后通过编码器 `self.encoder` 对嵌入结果进行编码，得到编码器的输出 `memory`。\n",
    "- `decode` 方法：通过调用目标语言嵌入层 `self.tgt_embed(tgt)` 将目标语言输入序列 `tgt` 进行嵌入操作，然后通过解码器 `self.decoder` 对嵌入结果进行解码，得到解码器的输出。\n",
    "- `forward` 方法：调用 `encode` 方法将源语言输入序列编码为 `memory`，然后调用 `decode` 方法将 `memory`、源语言掩码 `src_mask`、目标语言输入序列 `tgt` 和目标语言掩码 `tgt_mask` 传递给解码器，得到最终的解码结果。\n",
    "\n",
    "整体而言，这段代码定义了一个标准的编码器-解码器架构。它将编码器、解码器、嵌入层和生成器组合在一起，提供了对源语言和目标语言序列进行编码和解码的接口。编码器将源语言输入序列进行编码，生成编码器的输出 `memory`，解码器使用 `memory`、源语言掩码、目标语言输入序列和目标语言掩码进行解码，得到最终的解码结果。这段代码定义了一个标准的编码器-解码器（Encoder-Decoder）架构，作为其他模型的基础。\n",
    "\n",
    "在初始化函数中，`EncoderDecoder` 类接受五个参数：\n",
    "- `encoder`：编码器模型。\n",
    "- `decoder`：解码器模型。\n",
    "- `src_embed`：源语言嵌入层。\n",
    "- `tgt_embed`：目标语言嵌入层。\n",
    "- `generator`：生成器（输出层）。\n",
    "\n",
    "在初始化过程中，通过调用 `super(EncoderDecoder, self).__init__()` 构造父类的初始化函数。\n",
    "\n",
    "在前向传播函数中，输入包括源语言输入序列 `src`、目标语言输入序列 `tgt`，以及对应的掩码 `src_mask` 和 `tgt_mask`。\n",
    "\n",
    "- `encode` 方法：通过调用源语言嵌入层 `self.src_embed(src)` 将源语言输入序列 `src` 进行嵌入操作，然后通过编码器 `self.encoder` 对嵌入结果进行编码，得到编码器的输出 `memory`。\n",
    "- `decode` 方法：通过调用目标语言嵌入层 `self.tgt_embed(tgt)` 将目标语言输入序列 `tgt` 进行嵌入操作，然后通过解码器 `self.decoder` 对嵌入结果进行解码，得到解码器的输出。\n",
    "- `forward` 方法：调用 `encode` 方法将源语言输入序列编码为 `memory`，然后调用 `decode` 方法将 `memory`、源语言掩码 `src_mask`、目标语言输入序列 `tgt` 和目标语言掩码 `tgt_mask` 传递给解码器，得到最终的解码结果。\n",
    "\n",
    "整体而言，这段代码定义了一个标准的编码器-解码器架构。它将编码器、解码器、嵌入层和生成器组合在一起，提供了对源语言和目标语言序列进行编码和解码的接口。编码器将源语言输入序列进行编码，生成编码器的输出 `memory`，解码器使用 `memory`、源语言掩码、目标语言输入序列和目标语言掩码进行解码，得到最终的解码结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4ec31",
   "metadata": {
    "id": "NKGoH2RsTsqC",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9088113e-0449-457d-b18c-fab7fd39b994",
   "metadata": {},
   "source": [
    "这段代码定义了一个生成器（Generator）类，用于在Transformer模型中进行线性变换和softmax操作以生成输出。\n",
    "\n",
    "在初始化函数中，生成器接受两个参数：d_model 表示输入向量的维度，vocab 表示输出的词汇表大小。\n",
    "\n",
    "在前向传播函数中，输入 x 是一个表示模型的隐藏状态或特征的张量。通过对输入进行线性变换，通过 self.proj 层，将输入从隐藏维度（d_model）映射到输出词汇表大小的维度。\n",
    "\n",
    "然后，通过对线性变换后的结果应用 log_softmax 函数，并指定 dim=-1 来计算输出的对数softmax值。dim=-1 表示对最后一个维度进行softmax计算，即在这里是对输出词汇表的维度进行softmax。\n",
    "\n",
    "最后，将计算得到的对数softmax值作为函数的返回结果。\n",
    "\n",
    "综上所述，这段代码定义了一个简单的生成器类，它通过线性变换将输入映射到输出词汇表的维度，并使用softmax函数计算输出的对数概率值。这是Transformer模型中生成输出的一部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc49ad0",
   "metadata": {
    "id": "mOoEnF_jTsqC"
   },
   "source": [
    "\n",
    "The Transformer follows this overall architecture using stacked\n",
    "self-attention and point-wise, fully connected layers for both the\n",
    "encoder and decoder, shown in the left and right halves of Figure 1,\n",
    "respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c8af9f",
   "metadata": {
    "id": "oredWloYTsqC",
    "lines_to_next_cell": 2
   },
   "source": [
    "![](images/ModalNet-21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b52804",
   "metadata": {
    "id": "bh092NZBTsqD"
   },
   "source": [
    "## Encoder and Decoder Stacks\n",
    "\n",
    "### Encoder\n",
    "\n",
    "The encoder is composed of a stack of $N=6$ identical layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc63f9",
   "metadata": {
    "id": "2gxTApUYTsqD"
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4627cfd",
   "metadata": {
    "id": "xqVTz9MkTsqD"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ae4f8-54bd-489e-8ee6-f060d49c5ade",
   "metadata": {},
   "source": [
    "这段代码定义了一个编码器（Encoder）模块，它由多个相同类型的层堆叠而成。\n",
    "\n",
    "在初始化函数中，`Encoder` 类接受两个参数：`layer` 表示编码器中的单个层（Layer），`N` 表示层的数量。它通过调用父类 `nn.Module` 的 `__init__` 方法进行初始化，并使用 `clones` 函数复制 `layer` N 次，得到一个层的列表 `self.layers`。同时，创建了一个层归一化（LayerNorm）模块 `self.norm`，用于对编码器输出进行归一化。\n",
    "\n",
    "在前向传播函数中，输入 `x` 是编码器的输入张量，`mask` 是一个用于掩码的张量（可选）。它通过将输入张量 `x` 逐层传递给编码器中的每个层进行处理。\n",
    "\n",
    "具体来说，通过迭代遍历 `self.layers` 列表中的每个层，并将输入张量 `x` 传递给每个层进行计算。在每个层的计算过程中，可能还会使用 `mask` 对输入进行掩码处理。\n",
    "\n",
    "最后，通过 `self.norm(x)` 对编码器输出进行层归一化，将每个维度的特征进行归一化操作，以提高模型的训练稳定性和表示能力。\n",
    "\n",
    "综上所述，这段代码定义了一个编码器模块，它由多个层堆叠而成。在前向传播过程中，输入通过每个层进行逐层处理，并最终通过层归一化获得编码器的输出。编码器常用于Transformer模型等中，用于对输入序列进行特征提取和编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8962b0d7",
   "metadata": {
    "id": "GjAKgjGwTsqD"
   },
   "source": [
    "\n",
    "We employ a residual connection\n",
    "[(cite)](https://arxiv.org/abs/1512.03385) around each of the two\n",
    "sub-layers, followed by layer normalization\n",
    "[(cite)](https://arxiv.org/abs/1607.06450)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9c5a4d",
   "metadata": {
    "id": "3jKa_prZTsqE"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223dca42-81f0-4944-b327-9fe83cf92bfa",
   "metadata": {},
   "source": [
    "这段代码定义了一个 Layer Normalization（层归一化）的模块。\n",
    "\n",
    "在初始化函数中，`LayerNorm` 类接受两个参数：`features` 表示输入的特征维度，`eps` 是一个小的常数，用于防止除零错误。它通过调用父类 `nn.Module` 的 `__init__` 方法进行初始化，并定义了两个可学习的参数 `self.a_2` 和 `self.b_2`，它们分别对应于缩放（scale）和偏移（shift）操作。\n",
    "\n",
    "在前向传播函数中，输入 `x` 是一个表示模型的隐藏状态或特征的张量。通过对输入进行计算，实现了层归一化的操作。\n",
    "\n",
    "具体来说，首先计算输入 `x` 沿着最后一个维度的均值 `mean` 和标准差 `std`。这里使用了 `x.mean(-1, keepdim=True)` 和 `x.std(-1, keepdim=True)` 来计算均值和标准差，并保持维度一致。\n",
    "\n",
    "然后，通过 `(x - mean) / (std + self.eps)` 对输入进行归一化操作，将输入减去均值，除以标准差。这里使用了广播机制，确保计算在每个元素上都可以正确进行。\n",
    "\n",
    "最后，通过 `self.a_2 * (x - mean) / (std + self.eps) + self.b_2` 将归一化后的结果进行缩放和偏移，得到最终的层归一化结果。\n",
    "\n",
    "综上所述，这段代码定义了一个层归一化的模块，它对输入进行归一化操作，以及可学习的缩放和偏移操作。层归一化在神经网络中用于提高模型的训练稳定性和泛化性能，常用于Transformer等模型中的每个子层的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff9df3f",
   "metadata": {
    "id": "nXSJ3QYmTsqE"
   },
   "source": [
    "\n",
    "That is, the output of each sub-layer is $\\mathrm{LayerNorm}(x +\n",
    "\\mathrm{Sublayer}(x))$, where $\\mathrm{Sublayer}(x)$ is the function\n",
    "implemented by the sub-layer itself.  We apply dropout\n",
    "[(cite)](http://jmlr.org/papers/v15/srivastava14a.html) to the\n",
    "output of each sub-layer, before it is added to the sub-layer input\n",
    "and normalized.\n",
    "\n",
    "To facilitate these residual connections, all sub-layers in the\n",
    "model, as well as the embedding layers, produce outputs of dimension\n",
    "$d_{\\text{model}}=512$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2014803a",
   "metadata": {
    "id": "U1P7zI0eTsqE"
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b3f27b-7016-415a-aafb-6161b11cec52",
   "metadata": {},
   "source": [
    "这段代码定义了一个子层连接（SublayerConnection）模块，它包含一个残差连接（residual connection）和一个层归一化（LayerNorm）操作。\n",
    "\n",
    "在初始化函数中，`SublayerConnection` 类接受两个参数：`size` 表示输入的特征维度大小，`dropout` 是一个用于随机失活的概率。它通过调用父类 `nn.Module` 的 `__init__` 方法进行初始化，并创建了一个层归一化模块 `self.norm` 和一个 dropout 模块 `self.dropout`。\n",
    "\n",
    "在前向传播函数中，输入 `x` 是一个张量，表示来自上一层的输入。`sublayer` 是一个函数或模块，代表子层的计算过程。该函数或模块的输入和输出维度与 `x` 相同。\n",
    "\n",
    "具体来说，`forward` 函数实现了将残差连接应用于任意子层的操作。它首先对输入 `x` 进行层归一化操作，通过 `self.norm(x)` 得到归一化的结果。\n",
    "\n",
    "然后，通过将归一化后的结果传递给 `sublayer` 进行计算，并应用 dropout 操作，即 `self.dropout(sublayer(self.norm(x)))`。\n",
    "\n",
    "最后，通过将计算结果与输入 `x` 相加，实现残差连接，即 `x + self.dropout(sublayer(self.norm(x)))`。这里的加法操作将输入 `x` 与经过子层计算和残差连接后的结果相加，从而将子层的计算结果与原始输入进行融合。\n",
    "\n",
    "综上所述，这段代码定义了一个子层连接模块，它实现了残差连接和层归一化操作，用于将子层的计算结果与输入进行融合。这种设计常用于Transformer模型中的编码器和解码器中，以促进信息流动和提高模型性能。\n",
    "\n",
    "具体来说，`sublayer` 是一个表示子层的函数或模块，它将输入 `x` 经过一系列的计算得到子层的输出。而 `self.dropout` 是一个 dropout 模块，用于在训练过程中随机地将一部分元素置为零，以防止模型过拟合。\n",
    "\n",
    "在这里，通过应用 `self.dropout` 到 `sublayer(self.norm(x))`，实际上是对子层的计算结果进行了随机失活操作。dropout 操作以一定的概率将子层计算结果的某些元素置为零，从而减少模型对于特定元素的依赖，防止过拟合，并提高模型的泛化能力。\n",
    "\n",
    "需要注意的是，这段代码中的 `self.dropout` 是在 `SublayerConnection` 模块的初始化函数中定义的，它会在每次前向传播过程中被调用。因此，在每个子层的计算结果上应用 dropout 操作，可以在每个前向传播步骤中以一定的概率随机丢弃一些计算结果的元素。\n",
    "\n",
    "综上所述，通过在子层的计算结果上应用 `self.dropout`，可以对子层的输出进行随机失活操作，以增强模型的鲁棒性和泛化能力。这对于防止过拟合并提高模型性能非常有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba3da5",
   "metadata": {
    "id": "ML6oDlEqTsqE"
   },
   "source": [
    "\n",
    "Each layer has two sub-layers. The first is a multi-head\n",
    "self-attention mechanism, and the second is a simple, position-wise\n",
    "fully connected feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb8cf0",
   "metadata": {
    "id": "qYkUFr6GTsqE"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf79a5-5f1c-435d-98dd-e759d9cad49d",
   "metadata": {},
   "source": [
    "这段代码定义了编码器层（EncoderLayer）模块，它由自注意力机制和前馈神经网络组成。\n",
    "\n",
    "在初始化函数中，`EncoderLayer` 类接受四个参数：`size` 表示编码器层的输入和输出的特征维度大小，`self_attn` 表示自注意力机制的模块，`feed_forward` 表示前馈神经网络的模块，`dropout` 是一个用于随机失活的概率。它通过调用父类 `nn.Module` 的 `__init__` 方法进行初始化，并创建了两个 `SublayerConnection` 的实例 `self.sublayer`。\n",
    "\n",
    "在前向传播函数中，输入 `x` 是编码器层的输入张量，`mask` 是一个用于掩码的张量。按照图示中的连接方式，首先将输入 `x` 传递给第一个子层连接 `self.sublayer[0]`，其中包含了自注意力机制。在这个子层连接中，通过调用 `self.self_attn(x, x, x, mask)` 来对输入进行自注意力计算，获取编码器层的中间输出。\n",
    "\n",
    "然后，将中间输出传递给第二个子层连接 `self.sublayer[1]`，其中包含了前馈神经网络。在这个子层连接中，通过调用 `self.feed_forward` 对中间输出进行前馈神经网络的计算，得到编码器层的最终输出。\n",
    "\n",
    "最后，将最终输出作为函数的返回结果。\n",
    "\n",
    "综上所述，这段代码定义了一个编码器层模块，它由自注意力机制和前馈神经网络组成。在前向传播过程中，输入经过自注意力计算和前馈神经网络计算，以获取编码器层的输出。这种设计常用于Transformer模型的编码器中，用于对输入序列进行特征提取和编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c25de3f",
   "metadata": {
    "id": "7ecOQIhkTsqF"
   },
   "source": [
    "### Decoder\n",
    "\n",
    "The decoder is also composed of a stack of $N=6$ identical layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f6a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c8f46-8ddc-4fd1-8675-be746fcae826",
   "metadata": {},
   "source": [
    "这段代码定义了一个解码器（Decoder）模块，它由多个相同类型的层堆叠而成，并带有掩码（masking）操作。\n",
    "\n",
    "在初始化函数中，`Decoder` 类接受两个参数：`layer` 表示解码器中的单个层（Layer），`N` 表示层的数量。它通过调用父类 `nn.Module` 的 `__init__` 方法进行初始化，并使用 `clones` 函数复制 `layer` N 次，得到一个层的列表 `self.layers`。同时，创建了一个层归一化（LayerNorm）模块 `self.norm`，用于对解码器输出进行归一化。\n",
    "\n",
    "在前向传播函数中，输入 `x` 是解码器的输入张量，`memory` 是编码器的输出张量（记忆张量），`src_mask` 是一个用于对编码器输出进行掩码的张量，`tgt_mask` 是一个用于对解码器输入进行掩码的张量。\n",
    "\n",
    "具体来说，通过迭代遍历 `self.layers` 列表中的每个层，并将输入张量 `x`、记忆张量 `memory`、编码器输出的掩码 `src_mask`、解码器输入的掩码 `tgt_mask` 传递给每个层进行处理。\n",
    "\n",
    "在每个层的计算过程中，会利用 `memory` 中的编码器输出作为参考进行注意力计算，使用 `src_mask` 控制对编码器输出的注意力分布，并使用 `tgt_mask` 控制对解码器输入的注意力分布。\n",
    "\n",
    "最后，通过 `self.norm(x)` 对解码器的输出进行层归一化，将每个维度的特征进行归一化操作，以提高模型的训练稳定性和表示能力。\n",
    "\n",
    "综上所述，这段代码定义了一个解码器模块，它由多个层堆叠而成，并带有掩码操作。在前向传播过程中，输入通过每个层进行逐层处理，并最终通过层归一化获得解码器的输出。解码器常用于Transformer模型中，用于将编码器的输出解码为目标序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b7a0a1",
   "metadata": {
    "id": "dXlCB12pTsqF"
   },
   "source": [
    "\n",
    "In addition to the two sub-layers in each encoder layer, the decoder\n",
    "inserts a third sub-layer, which performs multi-head attention over\n",
    "the output of the encoder stack.  Similar to the encoder, we employ\n",
    "residual connections around each of the sub-layers, followed by\n",
    "layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ee274",
   "metadata": {
    "id": "M2hA1xFQTsqF"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d136543-796d-4299-aa99-eec654e20ffe",
   "metadata": {},
   "source": [
    "这段代码定义了一个解码器层（DecoderLayer）模块，它由自注意力机制、源注意力机制和前馈神经网络组成。\n",
    "\n",
    "在初始化函数中，`DecoderLayer` 类接受五个参数：`size` 表示解码器层的输入和输出的特征维度大小，`self_attn` 表示自注意力机制的模块，`src_attn` 表示源注意力机制的模块，`feed_forward` 表示前馈神经网络的模块，`dropout` 是一个用于随机失活的概率。它通过调用父类 `nn.Module` 的 `__init__` 方法进行初始化，并创建了三个 `SublayerConnection` 的实例 `self.sublayer`。\n",
    "\n",
    "在前向传播函数中，输入 `x` 是解码器层的输入张量，`memory` 是编码器的输出张量（记忆张量），`src_mask` 是一个用于对编码器输出进行掩码的张量，`tgt_mask` 是一个用于对解码器输入进行掩码的张量。\n",
    "\n",
    "具体来说，根据图示中的连接方式，首先将输入 `x` 传递给第一个子层连接 `self.sublayer[0]`，其中包含了自注意力机制。在这个子层连接中，通过调用 `self.self_attn(x, x, x, tgt_mask)` 来对输入进行自注意力计算，获取解码器层的中间输出。\n",
    "\n",
    "然后，将中间输出传递给第二个子层连接 `self.sublayer[1]`，其中包含了源注意力机制。在这个子层连接中，通过调用 `self.src_attn(x, m, m, src_mask)` 来对中间输出和记忆张量进行注意力计算，得到解码器层的中间输出。\n",
    "\n",
    "最后，将中间输出传递给第三个子层连接 `self.sublayer[2]`，其中包含了前馈神经网络。在这个子层连接中，通过调用 `self.feed_forward` 对中间输出进行前馈神经网络的计算，得到解码器层的最终输出。\n",
    "\n",
    "综上所述，这段代码定义了一个解码器层模块，它由自注意力机制、源注意力机制和前馈神经网络组成。在前向传播过程中，输入经过自注意力计算、源注意力计算和前馈神经网络计算，以获取解码器层的输出。这种设计常用于Transformer模型的解码器中，用于将编码器的输出解码为目标序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b04c48",
   "metadata": {
    "id": "FZz5rLl4TsqF"
   },
   "source": [
    "\n",
    "We also modify the self-attention sub-layer in the decoder stack to\n",
    "prevent positions from attending to subsequent positions.  This\n",
    "masking, combined with fact that the output embeddings are offset by\n",
    "one position, ensures that the predictions for position $i$ can\n",
    "depend only on the known outputs at positions less than $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0588bf",
   "metadata": {
    "id": "QN98O2l3TsqF"
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a63156-9c14-4804-898f-20961a876677",
   "metadata": {},
   "source": [
    "这段代码定义了一个函数 `subsequent_mask(size)`，用于生成一个遮蔽矩阵，用于在自注意力机制中屏蔽后续位置的信息。\n",
    "\n",
    "函数接受一个参数 `size`，表示生成遮蔽矩阵的大小。\n",
    "\n",
    "具体来说，首先通过 `(1, size, size)` 定义了遮蔽矩阵的形状 `attn_shape`，其中 `1` 表示批次大小为1（因为遮蔽矩阵在每个样本上都是相同的），`size` 表示遮蔽矩阵的高度和宽度。\n",
    "\n",
    "然后，通过 `torch.triu(torch.ones(attn_shape), diagonal=1)` 生成了一个上三角矩阵，该矩阵的主对角线及其以下的元素为1，其余元素为0。这样的矩阵正好可以用于屏蔽后续位置的信息。\n",
    "\n",
    "接下来，通过 `.type(torch.uint8)` 将矩阵的数据类型转换为无符号整型，以便在后续的逻辑中进行逻辑比较。\n",
    "\n",
    "最后，通过 `return subsequent_mask == 0` 将遮蔽矩阵与0进行逻辑比较，得到一个由True和False组成的布尔型遮蔽矩阵，其中True表示需要被遮蔽的位置，False表示不需要被遮蔽的位置。\n",
    "\n",
    "综上所述，这段代码定义了一个函数，用于生成一个遮蔽矩阵，该遮蔽矩阵可以在自注意力机制中屏蔽后续位置的信息，以防止当前位置获取未来位置的信息。这在Transformer模型等序列生成任务中非常有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8d50c2",
   "metadata": {
    "id": "Vg_f_w-PTsqG"
   },
   "source": [
    "\n",
    "> Below the attention mask shows the position each tgt word (row) is\n",
    "> allowed to look at (column). Words are blocked for attending to\n",
    "> future words during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38e12df",
   "metadata": {
    "id": "ht_FtgYAokC4"
   },
   "outputs": [],
   "source": [
    "def example_mask():\n",
    "    LS_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Subsequent Mask\": subsequent_mask(20)[0][x, y].flatten(),\n",
    "                    \"Window\": y,\n",
    "                    \"Masking\": x,\n",
    "                }\n",
    "            )\n",
    "            for y in range(20)\n",
    "            for x in range(20)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(LS_data)\n",
    "        .mark_rect()\n",
    "        .properties(height=250, width=250)\n",
    "        .encode(\n",
    "            alt.X(\"Window:O\"),\n",
    "            alt.Y(\"Masking:O\"),\n",
    "            alt.Color(\"Subsequent Mask:Q\", scale=alt.Scale(scheme=\"viridis\")),\n",
    "        )\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "show_example(example_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f50ef97-5ad8-4a5e-a18e-96c82d3e28cd",
   "metadata": {},
   "source": [
    "这段代码展示了如何使用 `subsequent_mask` 函数生成遮蔽矩阵，并将其可视化。\n",
    "\n",
    "首先，通过使用列表推导式生成了一个 Pandas DataFrame `LS_data`，其中包含了遮蔽矩阵的各个元素以及相应的行列索引信息。具体地，`y` 和 `x` 都遍历了 0 到 19 的范围，用于生成 20x20 的遮蔽矩阵。对于每个位置，将 `subsequent_mask(20)[0][x, y].flatten()` 的值作为 \"Subsequent Mask\" 列的元素，将 `y` 的值作为 \"Window\" 列的元素，将 `x` 的值作为 \"Masking\" 列的元素。最终，通过 `pd.concat` 将生成的 DataFrame 连接起来。\n",
    "\n",
    "`subsequent_mask(20)[0][x, y].flatten()`  这一句代码的含义是获取生成的遮蔽矩阵 `subsequent_mask(20)` 的某个位置上的元素值。首先，`subsequent_mask(20)` 调用了之前定义的 `subsequent_mask` 函数，并传入参数 `20`，生成一个遮蔽矩阵。这个遮蔽矩阵具有 `20x20` 的形状。然后，通过 `[0][x, y]` 的索引操作，从遮蔽矩阵的第一个元素（第一维）中，获取指定位置 `(x, y)` 处的元素。这里的 `x` 和 `y` 是之前在列表推导式中循环遍历的变量，代表遮蔽矩阵中的行和列索引。最后，通过 `.flatten()` 方法将获取的元素值展平为一维数组。这样做是为了方便后续使用，将遮蔽矩阵的元素值作为 \"Subsequent Mask\" 列的元素之一。这一句代码的含义是从生成的遮蔽矩阵中获取指定位置 `(x, y)` 处的元素值，并将其展平为一维数组。\n",
    "\n",
    "接下来，使用 Altair 库创建了一个图表对象。使用 `.mark_rect()` 指定图表中的图形类型为矩形，`.properties(height=250, width=250)` 设置图表的高度和宽度为 250 像素。然后，使用 `.encode()` 方法指定了数据在图表中的编码方式。`alt.X(\"Window:O\")` 将 \"Window\" 列作为 x 轴，`alt.Y(\"Masking:O\")` 将 \"Masking\" 列作为 y 轴，`alt.Color(\"Subsequent Mask:Q\", scale=alt.Scale(scheme=\"viridis\"))` 将 \"Subsequent Mask\" 列作为颜色编码，并使用 \"viridis\" 调色板进行着色。\n",
    "\n",
    "最后，通过 `.interactive()` 使图表具有交互性。\n",
    "\n",
    "函数 `show_example()` 被调用，用于显示生成的图表对象 `example_mask()`。\n",
    "\n",
    "综上所述，这段代码使用 `subsequent_mask` 函数生成遮蔽矩阵，并使用 Altair 库将其可视化为一个交互式的矩形图表，用于展示遮蔽矩阵中的不同位置和遮蔽情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781970bd",
   "metadata": {
    "id": "Qto_yg7BTsqG",
    "lines_to_next_cell": 2
   },
   "source": [
    "### Attention\n",
    "\n",
    "An attention function can be described as mapping a query and a set\n",
    "of key-value pairs to an output, where the query, keys, values, and\n",
    "output are all vectors.  The output is computed as a weighted sum of\n",
    "the values, where the weight assigned to each value is computed by a\n",
    "compatibility function of the query with the corresponding key.\n",
    "\n",
    "We call our particular attention \"Scaled Dot-Product Attention\".\n",
    "The input consists of queries and keys of dimension $d_k$, and\n",
    "values of dimension $d_v$.  We compute the dot products of the query\n",
    "with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax\n",
    "function to obtain the weights on the values.\n",
    "\n",
    "\n",
    "\n",
    "![](images/ModalNet-19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5448d6",
   "metadata": {
    "id": "EYJLWk6cTsqG"
   },
   "source": [
    "\n",
    "In practice, we compute the attention function on a set of queries\n",
    "simultaneously, packed together into a matrix $Q$.  The keys and\n",
    "values are also packed together into matrices $K$ and $V$.  We\n",
    "compute the matrix of outputs as:\n",
    "\n",
    "$$\n",
    "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dadf6f6",
   "metadata": {
    "id": "qsoVxS5yTsqG",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8acf39a-4ed0-404d-99ad-58ef464a977a",
   "metadata": {},
   "source": [
    "这段代码定义了一个函数 `attention`，用于计算缩放点积注意力（Scaled Dot Product Attention）。\n",
    "\n",
    "函数接受五个参数：\n",
    "- `query`: 查询张量，形状为 [batch_size, query_len, d_k]。\n",
    "- `key`: 键张量，形状为 [batch_size, key_len, d_k]。\n",
    "- `value`: 值张量，形状为 [batch_size, value_len, d_v]。\n",
    "- `mask`: 用于掩码的张量，形状为 [batch_size, query_len, key_len]，可以将特定位置的注意力分数屏蔽为负无穷大。\n",
    "- `dropout`: 可选的 dropout 模块，用于对注意力分布进行随机失活操作。\n",
    "\n",
    "具体来说，首先通过 `query.size(-1)` 获取查询张量的最后一个维度 `d_k`，即特征维度。\n",
    "\n",
    "然后，通过 `torch.matmul(query, key.transpose(-2, -1))` 进行点积操作，并除以 `math.sqrt(d_k)` 进行缩放，得到注意力分数张量 `scores`。此处使用了张量的转置操作 `key.transpose(-2, -1)`，将键张量的倒数第二个和倒数第一个维度进行转置，以适配点积运算。一般情况下， key的倒数第一维是向量的d_k, 倒数第二维是输入key的sequence的长度。\n",
    "\n",
    "接下来，如果存在掩码 `mask`，则使用 `scores.masked_fill(mask == 0, -1e9)` 将掩码中为0的位置对应的注意力分数置为负无穷大，以实现屏蔽效果。\n",
    "\n",
    "然后，通过 `scores.softmax(dim=-1)` 对注意力分数进行 softmax 操作，得到注意力权重张量 `p_attn`。注意力权重表示了每个位置的重要性权重。\n",
    "\n",
    "最后，如果提供了 `dropout`，则对注意力权重进行随机失活操作，即 `dropout(p_attn)`。\n",
    "\n",
    "函数返回通过注意力权重加权后的值张量 `torch.matmul(p_attn, value)`，以及注意力权重张量 `p_attn`。\n",
    "\n",
    "其中 `p_attn` 是注意力权重张量，其形状取决于输入的查询张量和键张量的形状。通常情况下，如果查询张量的形状是 [batch_size, query_len, d_k]，键张量的形状是 [batch_size, key_len, d_k]，那么注意力权重张量 `p_attn` 的形状为 [batch_size, query_len, key_len]。\n",
    "每个维度的含义如下：\n",
    "- 第一个维度 `batch_size`：表示批次的大小，即处理的样本数量。\n",
    "- 第二个维度 `query_len`：表示查询张量的长度，即查询序列的长度。\n",
    "- 第三个维度 `key_len`：表示键张量的长度，即键序列的长度。\n",
    "注意力权重张量 `p_attn` 中的每个元素表示了查询序列中的每个位置与键序列中的每个位置之间的注意力权重。它描述了查询序列中每个位置对键序列中的位置的重要性权重。可以通过索引 `p_attn[i, j, k]` 获取批次中第 `i` 个样本的查询位置 `j` 对键位置 `k` 的注意力权重值。`p_attn` 的形状为 [batch_size, query_len, key_len]，其中每个维度表示了对应的含义。\n",
    "\n",
    "综上所述，这段代码定义了一个用于计算缩放点积注意力的函数。它通过将查询、键和值张量进行点积运算，并使用缩放因子进行缩放，得到注意力分数。然后，根据掩码进行屏蔽操作，并通过 softmax 函数得到注意力权重。最后，对注意力权重进行可选的随机失活操作，并返回通过注意力权重加权后的值张量和注意力权重张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8016ab48",
   "metadata": {
    "id": "jUkpwu8kTsqG"
   },
   "source": [
    "\n",
    "The two most commonly used attention functions are additive\n",
    "attention [(cite)](https://arxiv.org/abs/1409.0473), and dot-product\n",
    "(multiplicative) attention.  Dot-product attention is identical to\n",
    "our algorithm, except for the scaling factor of\n",
    "$\\frac{1}{\\sqrt{d_k}}$. Additive attention computes the\n",
    "compatibility function using a feed-forward network with a single\n",
    "hidden layer.  While the two are similar in theoretical complexity,\n",
    "dot-product attention is much faster and more space-efficient in\n",
    "practice, since it can be implemented using highly optimized matrix\n",
    "multiplication code.\n",
    "\n",
    "\n",
    "While for small values of $d_k$ the two mechanisms perform\n",
    "similarly, additive attention outperforms dot product attention\n",
    "without scaling for larger values of $d_k$\n",
    "[(cite)](https://arxiv.org/abs/1703.03906). We suspect that for\n",
    "large values of $d_k$, the dot products grow large in magnitude,\n",
    "pushing the softmax function into regions where it has extremely\n",
    "small gradients (To illustrate why the dot products get large,\n",
    "assume that the components of $q$ and $k$ are independent random\n",
    "variables with mean $0$ and variance $1$.  Then their dot product,\n",
    "$q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i$, has mean $0$ and variance\n",
    "$d_k$.). To counteract this effect, we scale the dot products by\n",
    "$\\frac{1}{\\sqrt{d_k}}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4147d18",
   "metadata": {
    "id": "bS1FszhVTsqG",
    "lines_to_next_cell": 2
   },
   "source": [
    "![](images/ModalNet-20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf1118",
   "metadata": {
    "id": "TNtVyZ-pTsqH"
   },
   "source": [
    "\n",
    "Multi-head attention allows the model to jointly attend to\n",
    "information from different representation subspaces at different\n",
    "positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "$$\n",
    "\\mathrm{MultiHead}(Q, K, V) =\n",
    "    \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O \\\\\n",
    "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "$$\n",
    "\n",
    "Where the projections are parameter matrices $W^Q_i \\in\n",
    "\\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in\n",
    "\\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in\n",
    "\\mathbb{R}^{d_{\\text{model}} \\times d_v}$ and $W^O \\in\n",
    "\\mathbb{R}^{hd_v \\times d_{\\text{model}}}$.\n",
    "\n",
    "In this work we employ $h=8$ parallel attention layers, or\n",
    "heads. For each of these we use $d_k=d_v=d_{\\text{model}}/h=64$. Due\n",
    "to the reduced dimension of each head, the total computational cost\n",
    "is similar to that of single-head attention with full\n",
    "dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae79a1",
   "metadata": {
    "id": "D2LBMKCQTsqH"
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21845c-0bb8-462e-b146-4f90359200ae",
   "metadata": {},
   "source": [
    "这段代码定义了一个多头注意力（MultiHeadedAttention）模块，用于实现 Transformer 模型中的多头自注意力机制。\n",
    "\n",
    "在初始化函数中，`MultiHeadedAttention` 类接受三个参数：\n",
    "- `h`：表示头的数量，即多头注意力的个数。\n",
    "- `d_model`：表示输入和输出的特征维度。\n",
    "- `dropout`：可选参数，表示用于随机失活的概率。\n",
    "\n",
    "在初始化过程中，通过断言 `assert d_model % h == 0` 来确保特征维度 `d_model` 能够被头的数量 `h` 整除。接着，定义了每个头的特征维度 `self.d_k`，即 `d_model` 除以 `h` 的结果。然后，使用 `clones` 函数复制了四个线性层 `nn.Linear(d_model, d_model)`，分别用于处理查询、键、值和最终的线性变换。另外，还定义了一个用于进行随机失活的 `dropout` 模块。\n",
    "\n",
    "在前向传播函数中，输入包括 `query`、`key`、`value` 和可选的掩码 `mask`。首先，如果存在掩码 `mask`，则通过 `mask.unsqueeze(1)` 将其在第二维上进行扩展，以适应多头注意力的操作。\n",
    "\n",
    "接下来，根据论文中的步骤，执行以下操作：\n",
    "\n",
    "1) 执行线性变换，将 `query`、`key` 和 `value` 分别传入对应的线性层，并对每个线性层的输出进行维度变换，使其变为形状为 `[nbatches, seq_len, h, d_k]` 的张量。其中，`nbatches` 表示批次大小，`seq_len` 表示序列长度。\n",
    "\n",
    "2) 对变换后的张量应用注意力操作，调用 `attention` 函数。这里的 `attention` 函数就是之前解析的注意力计算函数，用于计算多头注意力的输出张量 `x` 和注意力权重张量 `self.attn`。\n",
    "\n",
    "3) 将输出张量 `x` 进行维度变换，将其转置为形状为 `[nbatches, seq_len, h * d_k]` 的连续张量。\n",
    "\n",
    "最后，通过调用 `self.linears[-1]` 对转置后的张量进行线性变换，得到最终的输出。\n",
    "\n",
    "整体而言，这段代码实现了多头注意力机制。它首先对查询、键和值进行线性变换和维度变换，然后应用注意力操作，最后通过线性变换得到输出。多头注意力可以提升模型的表示能力，允许模型在不同的表示子空间上进行并行计算，并且每个头可以关注序列中不同位置的相关信息。\n",
    "\n",
    "**注意， 下面这段代码具体是这样执行的**\n",
    "\n",
    "```python\n",
    "    query, key, value = [\n",
    "        lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        for lin, x in zip(self.linears, (query, key, value))\n",
    "    ]\n",
    "```\n",
    "这段代码用于将批量的查询（query）、键（key）和值（value）矩阵从特征维度 `d_model` 投影为多头注意力中的形状 `h * d_k`。\n",
    "\n",
    "具体而言，代码通过遍历三个线性层 `self.linears` 和三个输入张量 `(query, key, value)`，分别进行以下操作：\n",
    "\n",
    "1) 对每个输入张量 `x`，通过线性变换 `lin(x)` 得到投影后的结果。\n",
    "\n",
    "2) 使用 `.view(nbatches, -1, self.h, self.d_k)` 将投影结果进行形状重塑，将特征维度 `d_model` 分成 `h` 个头 `self.h`，每个头的特征维度为 `d_k`。\n",
    "\n",
    "3) 最后，通过 `.transpose(1, 2)` 进行维度转置，将头维度 `self.h` 和序列维度 `seq_len` 进行交换，以满足多头注意力的计算需求。\n",
    "\n",
    "经过投影之后，`query`、`key` 和 `value` 的形状会发生变化。假设原始输入的形状为：\n",
    "- `query`: [batch_size, query_len, d_model]\n",
    "- `key`: [batch_size, key_len, d_model]\n",
    "- `value`: [batch_size, value_len, d_model]\n",
    "\n",
    "经过投影后，它们的形状变为：\n",
    "- `query`: [batch_size,h, query_len, d_k]\n",
    "- `key`: [batch_size, h, key_len, d_k]\n",
    "- `value`: [batch_size, h, value_len, d_k]\n",
    "\n",
    "其中，\n",
    "- `batch_size` 表示批次大小，即处理的样本数量。\n",
    "- `query_len` 表示查询序列的长度。\n",
    "- `key_len` 表示键序列的长度。\n",
    "- `value_len` 表示值序列的长度。\n",
    "- `h` 表示头的数量，即多头注意力的个数。\n",
    "- `d_k` 表示每个头的特征维度。\n",
    "\n",
    "投影之后，每个输入矩阵在特征维度 `d_model` 上被分为 `h` 个头，每个头的特征维度为 `d_k`。维度变换后，头维度 `h` 位于第三个维度上，即在形状中的索引位置为 2。这样的形状安排使得后续的多头注意力计算可以并行处理不同的头。\n",
    "\n",
    "综上所述，经过投影后的 `query`、`key` 和 `value` 是具有多头注意力形状的张量。每个矩阵都包含了多个头，每个头具有特定的特征维度。\n",
    "\n",
    "总结起来，这段代码通过线性变换和形状重塑的操作，将输入的查询、键和值矩阵从特征维度 `d_model` 投影为多头注意力中的形状 `h * d_k`，为接下来的多头注意力计算做准备。每个头的特征维度为 `d_k`，总共有 `h` 个头。通过维度转置，确保输入张量在头和序列维度上的顺序满足多头注意力计算的要求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e54d46",
   "metadata": {
    "id": "EDRba3J3TsqH"
   },
   "source": [
    "### Applications of Attention in our Model\n",
    "\n",
    "The Transformer uses multi-head attention in three different ways:\n",
    "1) In \"encoder-decoder attention\" layers, the queries come from the\n",
    "previous decoder layer, and the memory keys and values come from the\n",
    "output of the encoder.  This allows every position in the decoder to\n",
    "attend over all positions in the input sequence.  This mimics the\n",
    "typical encoder-decoder attention mechanisms in sequence-to-sequence\n",
    "models such as [(cite)](https://arxiv.org/abs/1609.08144).\n",
    "\n",
    "\n",
    "2) The encoder contains self-attention layers.  In a self-attention\n",
    "layer all of the keys, values and queries come from the same place,\n",
    "in this case, the output of the previous layer in the encoder.  Each\n",
    "position in the encoder can attend to all positions in the previous\n",
    "layer of the encoder.\n",
    "\n",
    "\n",
    "3) Similarly, self-attention layers in the decoder allow each\n",
    "position in the decoder to attend to all positions in the decoder up\n",
    "to and including that position.  We need to prevent leftward\n",
    "information flow in the decoder to preserve the auto-regressive\n",
    "property.  We implement this inside of scaled dot-product attention\n",
    "by masking out (setting to $-\\infty$) all values in the input of the\n",
    "softmax which correspond to illegal connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a967168",
   "metadata": {
    "id": "M-en97_GTsqH"
   },
   "source": [
    "## Position-wise Feed-Forward Networks\n",
    "\n",
    "In addition to attention sub-layers, each of the layers in our\n",
    "encoder and decoder contains a fully connected feed-forward network,\n",
    "which is applied to each position separately and identically.  This\n",
    "consists of two linear transformations with a ReLU activation in\n",
    "between.\n",
    "\n",
    "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$\n",
    "\n",
    "While the linear transformations are the same across different\n",
    "positions, they use different parameters from layer to\n",
    "layer. Another way of describing this is as two convolutions with\n",
    "kernel size 1.  The dimensionality of input and output is\n",
    "$d_{\\text{model}}=512$, and the inner-layer has dimensionality\n",
    "$d_{ff}=2048$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fa5069",
   "metadata": {
    "id": "6HHCemCxTsqH"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424f264a-6233-49b3-a57f-722c72233fdb",
   "metadata": {},
   "source": [
    "这段代码定义了一个位置前馈神经网络（Positionwise Feed-Forward）模块，用于实现 Transformer 模型中的前馈神经网络层。\n",
    "\n",
    "在初始化函数中，`PositionwiseFeedForward` 类接受三个参数：\n",
    "- `d_model`：表示输入和输出的特征维度。\n",
    "- `d_ff`：表示前馈神经网络隐藏层的特征维度。\n",
    "- `dropout`：可选参数，表示用于随机失活的概率。\n",
    "\n",
    "在初始化过程中，通过两个线性层 `nn.Linear(d_model, d_ff)` 和 `nn.Linear(d_ff, d_model)` 来构建前馈神经网络的两个层。其中，第一个线性层将输入特征维度 `d_model` 转换为隐藏层特征维度 `d_ff`，第二个线性层将隐藏层特征维度 `d_ff` 转换回输入特征维度 `d_model`。另外，还定义了一个用于随机失活的 `dropout` 模块。\n",
    "\n",
    "在前向传播函数中，输入为 `x`，表示经过自注意力层处理后的张量。首先，通过 `self.w_1(x)` 将输入张量 `x` 输入到第一个线性层中，然后应用 ReLU 激活函数。接着，通过 `self.dropout` 对激活后的张量进行随机失活操作。最后，将随机失活后的张量输入到第二个线性层 `self.w_2` 中，并返回其结果。\n",
    "\n",
    "整体而言，这段代码实现了位置前馈神经网络层。它通过两个线性层和激活函数（ReLU）实现了非线性变换，从而对输入进行维度变换和特征提取。通过随机失活操作，有助于减少过拟合。前馈神经网络层在 Transformer 模型中被用于对序列中的每个位置进行独立的特征映射，增强了模型的表达能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec9cfa3",
   "metadata": {
    "id": "dR1YM520TsqH"
   },
   "source": [
    "## Embeddings and Softmax\n",
    "\n",
    "Similarly to other sequence transduction models, we use learned\n",
    "embeddings to convert the input tokens and output tokens to vectors\n",
    "of dimension $d_{\\text{model}}$.  We also use the usual learned\n",
    "linear transformation and softmax function to convert the decoder\n",
    "output to predicted next-token probabilities.  In our model, we\n",
    "share the same weight matrix between the two embedding layers and\n",
    "the pre-softmax linear transformation, similar to\n",
    "[(cite)](https://arxiv.org/abs/1608.05859). In the embedding layers,\n",
    "we multiply those weights by $\\sqrt{d_{\\text{model}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dcafa9",
   "metadata": {
    "id": "pyrChq9qTsqH"
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926737b7-b39b-417c-adcc-aca6ce7c3936",
   "metadata": {},
   "source": [
    "这段代码定义了一个嵌入层（Embeddings），用于将输入的离散化的标记（例如单词）转换为连续的向量表示，以便在 Transformer 模型中进行处理。\n",
    "\n",
    "在初始化函数中，`Embeddings` 类接受两个参数：\n",
    "- `d_model`：表示嵌入向量的维度，即每个标记被转换为的连续向量的维度。\n",
    "- `vocab`：表示词汇表的大小，即不同标记的数量。\n",
    "\n",
    "在初始化过程中，通过调用 `nn.Embedding(vocab, d_model)` 创建了一个嵌入层 `self.lut`，它将输入的标记索引映射为对应的嵌入向量。同时，还记录了嵌入向量的维度 `self.d_model`。\n",
    "\n",
    "在前向传播函数中，输入为 `x`，它是一个表示标记索引的张量。通过 `self.lut(x)` 将输入张量 `x` 输入到嵌入层中，将每个标记索引转换为对应的嵌入向量。最后，将嵌入向量乘以 `math.sqrt(self.d_model)` 进行缩放，以满足 Transformer 模型的数学约束（根据论文中的建议，对嵌入向量进行缩放可以更好地适应模型的训练）。\n",
    "\n",
    "整体而言，这段代码实现了一个嵌入层，用于将离散化的标记映射为连续的向量表示。嵌入层在 Transformer 模型中起着重要的作用，它将离散的输入转换为连续的表示，捕捉标记之间的语义关系，并提供给后续的模型进行处理和学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36421af",
   "metadata": {
    "id": "vOkdui-cTsqH"
   },
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Since our model contains no recurrence and no convolution, in order\n",
    "for the model to make use of the order of the sequence, we must\n",
    "inject some information about the relative or absolute position of\n",
    "the tokens in the sequence.  To this end, we add \"positional\n",
    "encodings\" to the input embeddings at the bottoms of the encoder and\n",
    "decoder stacks.  The positional encodings have the same dimension\n",
    "$d_{\\text{model}}$ as the embeddings, so that the two can be summed.\n",
    "There are many choices of positional encodings, learned and fixed\n",
    "[(cite)](https://arxiv.org/pdf/1705.03122.pdf).\n",
    "\n",
    "In this work, we use sine and cosine functions of different frequencies:\n",
    "\n",
    "$$PE_{(pos,2i)} = \\sin(pos / 10000^{2i/d_{\\text{model}}})$$\n",
    "\n",
    "$$PE_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d_{\\text{model}}})$$\n",
    "\n",
    "where $pos$ is the position and $i$ is the dimension.  That is, each\n",
    "dimension of the positional encoding corresponds to a sinusoid.  The\n",
    "wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot\n",
    "2\\pi$.  We chose this function because we hypothesized it would\n",
    "allow the model to easily learn to attend by relative positions,\n",
    "since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a\n",
    "linear function of $PE_{pos}$.\n",
    "\n",
    "In addition, we apply dropout to the sums of the embeddings and the\n",
    "positional encodings in both the encoder and decoder stacks.  For\n",
    "the base model, we use a rate of $P_{drop}=0.1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd23343",
   "metadata": {
    "id": "zaHGD4yJTsqH"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23980ed-d53e-4d03-a549-3918bbffd2c9",
   "metadata": {},
   "source": [
    "这段代码定义了一个位置编码（Positional Encoding）模块，用于为输入序列中的每个位置添加位置信息。\n",
    "\n",
    "在初始化函数中，`PositionalEncoding` 类接受三个参数：\n",
    "- `d_model`：表示输入和输出的特征维度。\n",
    "- `dropout`：表示用于随机失活的概率。\n",
    "- `max_len`：表示序列的最大长度，默认为 5000。\n",
    "\n",
    "在初始化过程中，通过计算位置编码，构建了一个形状为 `[max_len, d_model]` 的位置编码张量 `pe`。具体计算方法如下：\n",
    "\n",
    "1) 创建一个全零张量 `pe`，形状为 `[max_len, d_model]`，其中 `max_len` 是序列的最大长度，`d_model` 是特征维度。\n",
    "\n",
    "2) 创建一个形状为 `[max_len, 1]` 的位置张量 `position`，其中每个元素表示序列中的位置索引。\n",
    "\n",
    "3) 创建一个形状为 `[d_model/2]` 的除法项 `div_term`，其中每个元素计算为 `exp(-log(10000) * 2i / d_model)`，其中 `i` 表示元素索引。\n",
    "\n",
    "4) 使用 `torch.sin` 和 `torch.cos` 函数计算正弦和余弦值，并将它们分别应用于位置张量乘以除法项的结果。通过广播机制，将正弦和余弦值分别填充到位置编码张量的奇数和偶数列上。\n",
    "\n",
    "5) 最后，通过 `unsqueeze(0)` 在位置编码张量的最前面添加一个维度，以匹配输入张量的批次维度。\n",
    "\n",
    "在前向传播函数中，输入为 `x`，表示经过嵌入层处理后的张量。将位置编码张量 `self.pe` 的前 `x.size(1)` 个位置编码加到输入张量 `x` 上，并应用随机失活操作。这样，位置编码就会与输入特征相加，提供每个位置的位置信息。\n",
    "\n",
    "整体而言，这段代码实现了位置编码模块，用于为输入序列添加位置信息。位置编码通过正弦和余弦函数计算位置相关的编码值，并与输入张量相加。位置编码允许模型根据位置信息进行建模，有助于捕捉序列中元素之间的顺序关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c44083",
   "metadata": {
    "id": "EfHacTJLTsqH"
   },
   "source": [
    "\n",
    "> Below the positional encoding will add in a sine wave based on\n",
    "> position. The frequency and offset of the wave is different for\n",
    "> each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b025c310",
   "metadata": {
    "id": "rnvHk_1QokC6",
    "type": "example"
   },
   "outputs": [],
   "source": [
    "def example_positional():\n",
    "    pe = PositionalEncoding(20, 0)\n",
    "    y = pe.forward(torch.zeros(1, 100, 20))\n",
    "\n",
    "    data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"embedding\": y[0, :, dim],\n",
    "                    \"dimension\": dim,\n",
    "                    \"position\": list(range(100)),\n",
    "                }\n",
    "            )\n",
    "            for dim in [4, 5, 6, 7]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(data)\n",
    "        .mark_line()\n",
    "        .properties(width=800)\n",
    "        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "show_example(example_positional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2a7839",
   "metadata": {
    "id": "g8rZNCrzTsqI"
   },
   "source": [
    "\n",
    "We also experimented with using learned positional embeddings\n",
    "[(cite)](https://arxiv.org/pdf/1705.03122.pdf) instead, and found\n",
    "that the two versions produced nearly identical results.  We chose\n",
    "the sinusoidal version because it may allow the model to extrapolate\n",
    "to sequence lengths longer than the ones encountered during\n",
    "training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed3f91",
   "metadata": {
    "id": "iwNKCzlyTsqI"
   },
   "source": [
    "## Full Model\n",
    "\n",
    "> Here we define a function from hyperparameters to a full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3404f927",
   "metadata": {
    "id": "mPe1ES0UTsqI"
   },
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
    "):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab),\n",
    "    )\n",
    "\n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b311c26-a528-4272-a320-68dcc20a624a",
   "metadata": {},
   "source": [
    "这段代码定义了一个帮助函数 `make_model`，用于根据给定的超参数构建一个完整的 Transformer 模型。\n",
    "\n",
    "在函数参数中，传入了以下超参数：\n",
    "- `src_vocab`：源语言词汇表的大小。\n",
    "- `tgt_vocab`：目标语言词汇表的大小。\n",
    "- `N`：编码器和解码器中的层数。\n",
    "- `d_model`：模型的特征维度。\n",
    "- `d_ff`：前馈神经网络隐藏层的特征维度。\n",
    "- `h`：多头注意力中的头数。\n",
    "- `dropout`：用于随机失活的概率。\n",
    "\n",
    "在函数实现中，首先进行了 `copy.deepcopy` 操作，用于深拷贝对象，保证模型的参数共享与独立性。\n",
    "\n",
    "接下来，通过调用 `MultiHeadedAttention` 类创建了多头注意力层 `attn`，并通过调用 `PositionwiseFeedForward` 类创建了位置前馈神经网络层 `ff`，以及通过调用 `PositionalEncoding` 类创建了位置编码层 `position`。\n",
    "\n",
    "然后，通过将以上创建的模块组合在一起，构建了一个完整的 Transformer 模型 `model`。该模型由以下组件组成：\n",
    "- 编码器（`Encoder`）：由 `N` 个相同的编码器层（`EncoderLayer`）堆叠而成。\n",
    "- 解码器（`Decoder`）：由 `N` 个相同的解码器层（`DecoderLayer`）堆叠而成。\n",
    "- 源语言嵌入层和位置编码层：通过调用 `Embeddings` 类和 `PositionalEncoding` 类分别创建。\n",
    "- 目标语言嵌入层和位置编码层：通过调用 `Embeddings` 类和 `PositionalEncoding` 类分别创建。\n",
    "- 生成器（`Generator`）：用于生成目标语言词汇的线性变换层。\n",
    "\n",
    "最后，通过遍历模型的参数，使用 Glorot / fan_avg 初始化方法（也称为 Xavier 初始化方法）对模型的参数进行初始化。\n",
    "\n",
    "总结起来，这段代码通过给定的超参数构建了一个完整的 Transformer 模型。该模型包括编码器、解码器、嵌入层、位置编码层和生成器。这个函数提供了一种方便的方式来创建 Transformer 模型，并使用 Glorot / fan_avg 初始化方法对参数进行初始化，以确保模型在训练过程中的有效性和准确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d012d0c7",
   "metadata": {},
   "source": [
    "## Inference:\n",
    "\n",
    "> Here we make a forward step to generate a prediction of the\n",
    "model. We try to use our transformer to memorize the input. As you\n",
    "will see the output is randomly generated due to the fact that the\n",
    "model is not trained yet. In the next tutorial we will build the\n",
    "training function and try to train our model to memorize the numbers\n",
    "from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a5a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test():\n",
    "    test_model = make_model(11, 11, 2)\n",
    "    test_model.eval()\n",
    "    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "    src_mask = torch.ones(1, 1, 10)\n",
    "\n",
    "    memory = test_model.encode(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).type_as(src)\n",
    "\n",
    "    for i in range(9):\n",
    "        out = test_model.decode(\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        prob = test_model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "\n",
    "    print(\"Example Untrained Model Prediction:\", ys)\n",
    "\n",
    "\n",
    "def run_tests():\n",
    "    for _ in range(10):\n",
    "        inference_test()\n",
    "\n",
    "\n",
    "show_example(run_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e3d0e9-98d9-4d2a-9f0d-3a6009e40833",
   "metadata": {},
   "source": [
    "这段代码包含了两个函数 `inference_test()` 和 `run_tests()`，用于进行模型的推理测试。\n",
    "\n",
    "函数 `inference_test()`：\n",
    "- 创建一个测试模型 `test_model`，通过调用 `make_model(11, 11, 2)` 创建一个模型实例，其中源语言和目标语言的词汇表大小都为 11，层数为 2。\n",
    "- 将模型设为评估模式，即调用 `test_model.eval()`。\n",
    "- 定义一个源语言序列 `src`，其形状为 `[1, 10]`，内容为 `[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]`。\n",
    "- 定义源语言掩码 `src_mask`，形状为 `[1, 1, 10]`，全为 1。\n",
    "- 使用模型的 `encode` 方法将源语言序列 `src` 编码为 `memory`。\n",
    "- 初始化目标语言序列 `ys`，形状为 `[1, 1]`，元素全为 0，与源语言序列的数据类型相同。\n",
    "- 进行循环遍历，迭代次数为 9，用于生成目标语言序列。\n",
    "  - 使用模型的 `decode` 方法对 `memory` 进行解码，得到解码结果 `out`。\n",
    "  - 使用解码结果 `out` 的最后一个时间步的输出通过模型的生成器（输出层）得到概率分布 `prob`。\n",
    "  - 使用 `torch.max` 函数找到概率最高的单词，并将其作为下一个时间步的输入单词。\n",
    "  - 将下一个单词添加到目标语言序列 `ys` 中。\n",
    "- 打印输出结果，即模型的预测结果。\n",
    "\n",
    "函数 `run_tests()`：\n",
    "- 进行 10 次 `inference_test()` 的循环运行，用于多次测试模型的推理能力。\n",
    "\n",
    "最后，通过调用 `show_example(run_tests)` 函数来展示运行测试的结果。\n",
    "\n",
    "总体而言，这段代码用于测试模型的推理过程。通过给定的源语言序列，使用训练好的模型进行解码，生成目标语言序列的预测结果，并打印输出。通过多次运行 `inference_test()` 函数来进行推理测试，以验证模型的预测能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd11c1a",
   "metadata": {},
   "source": [
    "# Part 2: Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e324d",
   "metadata": {
    "id": "05s6oT9fTsqI"
   },
   "source": [
    "# Training\n",
    "\n",
    "This section describes the training regime for our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdfba89",
   "metadata": {
    "id": "fTxlofs4TsqI"
   },
   "source": [
    "\n",
    "> We stop for a quick interlude to introduce some of the tools\n",
    "> needed to train a standard encoder decoder model. First we define a\n",
    "> batch object that holds the src and target sentences for training,\n",
    "> as well as constructing the masks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd4ff43",
   "metadata": {
    "id": "G7SkCenXTsqI"
   },
   "source": [
    "## Batches and Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5a0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\"\"\"\n",
    "\n",
    "    def __init__(self, src, tgt=None, pad=2):  # 2 = <blank>\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1]\n",
    "            self.tgt_y = tgt[:, 1:]\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(\n",
    "            tgt_mask.data\n",
    "        )\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d69c45-c07d-4e10-8e32-d3c65d31b2f4",
   "metadata": {},
   "source": [
    "这段代码定义了一个名为 `Batch` 的类，用于在训练过程中存储一个数据批次及其相应的掩码。\n",
    "\n",
    "在类的初始化函数中，接受三个参数：\n",
    "- `src`：源语言数据序列。\n",
    "- `tgt`：目标语言数据序列（可选）。\n",
    "- `pad`：用于填充的特殊符号的索引（默认为 2）。\n",
    "\n",
    "在初始化过程中，执行以下操作：\n",
    "- 将源语言数据序列 `src` 存储到 `self.src` 中。\n",
    "- 通过比较 `src` 和 `pad` 的值，创建源语言数据序列的掩码 `self.src_mask`。掩码将非填充符号设为 1，填充符号设为 0，并在倒数第二个维度上添加一个维度，以与数据序列的维度对齐。\n",
    "\n",
    "如果目标语言数据序列 `tgt` 不为空，则还执行以下操作：\n",
    "- 将目标语言数据序列 `tgt` 去除最后一个时间步的标记，存储到 `self.tgt` 中。\n",
    "- 将目标语言数据序列 `tgt` 去除第一个时间步的标记，存储到 `self.tgt_y` 中。这样可以获得目标语言数据序列的标签。\n",
    "- 通过调用 `make_std_mask` 静态方法创建目标语言数据序列的掩码 `self.tgt_mask`。该方法用于创建一个掩码，用于隐藏填充符号和未来的单词。首先将非填充符号设为 1，然后通过调用 `subsequent_mask` 函数创建一个掩码，将未来的单词设为 0。最后，将两个掩码按位与运算，得到最终的目标语言数据序列掩码。\n",
    "- 使用 `(self.tgt_y != pad).data.sum()` 计算目标语言数据序列中非填充符号的数量，存储到 `self.ntokens` 中。\n",
    "\n",
    "`make_std_mask` 静态方法用于创建一个掩码，用于隐藏填充符号和未来的单词。它接受两个参数：\n",
    "- `tgt`：目标语言数据序列。\n",
    "- `pad`：用于填充的特殊符号的索引。\n",
    "\n",
    "在方法内部，执行以下操作：\n",
    "- 将目标语言数据序列中非填充符号设为 1，填充符号设为 0，创建目标语言数据序列的掩码 `tgt_mask`。并在倒数第二个维度上添加一个维度，以与数据序列的维度对齐。\n",
    "- 通过调用 `subsequent_mask` 函数创建一个掩码，将未来的单词设为 0。\n",
    "- 将两个掩码按位与运算，得到最终的目标语言数据序列掩码，并返回。\n",
    "\n",
    "总结起来，这段代码定义了一个 `Batch` 类，用于存储训练过程中的数据批次及其相应的掩码。通过类的初始化函数，可以根据源语言数据序列和目标语言数据序列创建批次对象，并生成相应的掩码。这个类提供了方便的方法来处理数据批次，并生成目标语言数据序列的掩码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397d696b",
   "metadata": {
    "id": "cKkw5GjLTsqI"
   },
   "source": [
    "\n",
    "> Next we create a generic training and scoring function to keep\n",
    "> track of loss. We pass in a generic loss compute function that\n",
    "> also handles parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9930b",
   "metadata": {
    "id": "Q8zzeUc0TsqJ"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba65f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState:\n",
    "    \"\"\"Track number of steps, examples, and tokens processed\"\"\"\n",
    "\n",
    "    step: int = 0  # Steps in the current epoch\n",
    "    accum_step: int = 0  # Number of gradient accumulation steps\n",
    "    samples: int = 0  # total # of examples used\n",
    "    tokens: int = 0  # total # of tokens processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e48a40-31bf-46d1-9496-dc2c35e8cb1a",
   "metadata": {},
   "source": [
    "这段代码定义了一个名为 `TrainState` 的类，用于跟踪训练过程中处理的步数、样本数和标记（tokens）数。\n",
    "\n",
    "类中定义了以下属性：\n",
    "- `step`：当前训练周期内的步数。\n",
    "- `accum_step`：梯度累积的步数。\n",
    "- `samples`：使用的总样本数。\n",
    "- `tokens`：处理的总标记数。\n",
    "\n",
    "这些属性用于记录训练过程中的统计信息，例如用于确定当前训练周期的步数、梯度累积的步数、处理的样本数和标记数。通过访问这些属性，可以追踪训练过程中的进度和性能指标。\n",
    "\n",
    "需要注意的是，这段代码使用了 Python 3.6+ 的类型注解语法，声明了属性的类型。属性的默认值在这里都被设置为整数类型，初始值为 0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112389f1",
   "metadata": {
    "id": "2HAZD3hiTsqJ"
   },
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    data_iter,\n",
    "    model,\n",
    "    loss_compute,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    mode=\"train\",\n",
    "    accum_iter=1,\n",
    "    train_state=TrainState(),\n",
    "):\n",
    "    \"\"\"Train a single epoch\"\"\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    n_accum = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(\n",
    "            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n",
    "        )\n",
    "        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n",
    "        # loss_node = loss_node / accum_iter\n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            loss_node.backward()\n",
    "            train_state.step += 1\n",
    "            train_state.samples += batch.src.shape[0]\n",
    "            train_state.tokens += batch.ntokens\n",
    "            if i % accum_iter == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                n_accum += 1\n",
    "                train_state.accum_step += 1\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                (\n",
    "                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n",
    "                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n",
    "                )\n",
    "                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)\n",
    "            )\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "        del loss\n",
    "        del loss_node\n",
    "    return total_loss / total_tokens, train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1826ce-a5a6-445b-985b-87d41ff02b04",
   "metadata": {},
   "source": [
    "这段代码定义了一个名为 `run_epoch` 的函数，用于训练一个周期（epoch）的模型。\n",
    "\n",
    "函数接受以下参数：\n",
    "- `data_iter`：数据迭代器，用于生成训练数据批次。\n",
    "- `model`：模型实例，用于进行前向计算。\n",
    "- `loss_compute`：损失计算对象，用于计算损失。\n",
    "- `optimizer`：优化器，用于更新模型参数。\n",
    "- `scheduler`：学习率调度器，用于调整学习率。\n",
    "- `mode`：模式，可选值为 `\"train\"`、`\"train+log\"` 或 `\"eval\"`，表示当前的运行模式，默认为 `\"train\"`。\n",
    "- `accum_iter`：梯度累积的迭代次数，默认为 1，表示不进行梯度累积。\n",
    "- `train_state`：训练状态对象，用于记录训练过程中的统计信息。\n",
    "\n",
    "函数内部执行以下操作：\n",
    "- 初始化一些变量，包括 `start`（开始时间）、`total_tokens`（总标记数）和 `total_loss`（总损失）等。\n",
    "- 迭代遍历数据迭代器，获取数据批次。\n",
    "- 调用模型的前向计算方法 `forward`，传入批次数据，得到模型的输出 `out`。\n",
    "- 调用损失计算对象的 `loss_compute` 方法，传入模型输出、目标语言序列和标记数，计算损失和损失节点（用于梯度反向传播）。\n",
    "- 如果当前模式是训练模式（\"train\" 或 \"train+log\"）：\n",
    "  - 执行损失节点的反向传播。\n",
    "  - 更新训练状态中的步数、样本数和标记数等统计信息。\n",
    "  - 如果达到了梯度累积的迭代次数（`i % accum_iter == 0`）：\n",
    "    - 执行优化器的参数更新。\n",
    "    - 清空梯度，以便进行下一轮的累积梯度计算。\n",
    "    - 更新训练状态中的梯度累积步数。\n",
    "  - 执行学习率调度器的更新。\n",
    "- 累加总损失和总标记数。\n",
    "- 更新当前周期内处理的标记数。\n",
    "- 如果满足打印日志的条件（`i % 40 == 1`），且当前模式是训练模式（\"train\" 或 \"train+log\"）：\n",
    "  - 获取当前优化器的学习率。\n",
    "  - 计算经过一定时间间隔的标记速度。\n",
    "  - 打印日志，包括当前周期的步数、梯度累积步数、损失、标记速度和学习率等信息。\n",
    "- 清空损失和损失节点，释放内存。\n",
    "- 返回总损失除以总标记数得到的平均损失，以及更新后的训练状态对象。\n",
    "\n",
    "总结起来，这段代码实现了一个训练周期的函数 `run_epoch`，通过循环迭代数据批次，进行模型的前向计算、损失计算、梯度反向传播和参数更新等操作。同时，根据指定的模式和梯度累积设置，记录训练过程中的统计信息，并打印训练日志。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c016ef77",
   "metadata": {
    "id": "aB1IF0foTsqJ"
   },
   "source": [
    "## Training Data and Batching\n",
    "\n",
    "We trained on the standard WMT 2014 English-German dataset\n",
    "consisting of about 4.5 million sentence pairs.  Sentences were\n",
    "encoded using byte-pair encoding, which has a shared source-target\n",
    "vocabulary of about 37000 tokens. For English-French, we used the\n",
    "significantly larger WMT 2014 English-French dataset consisting of\n",
    "36M sentences and split tokens into a 32000 word-piece vocabulary.\n",
    "\n",
    "\n",
    "Sentence pairs were batched together by approximate sequence length.\n",
    "Each training batch contained a set of sentence pairs containing\n",
    "approximately 25000 source tokens and 25000 target tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383c0a89",
   "metadata": {
    "incorrectly_encoded_metadata": "id=\"F1mTQatiTsqJ\" jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "## Hardware and Schedule\n",
    "\n",
    "We trained our models on one machine with 8 NVIDIA P100 GPUs.  For\n",
    "our base models using the hyperparameters described throughout the\n",
    "paper, each training step took about 0.4 seconds.  We trained the\n",
    "base models for a total of 100,000 steps or 12 hours. For our big\n",
    "models, step time was 1.0 seconds.  The big models were trained for\n",
    "300,000 steps (3.5 days)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5080042",
   "metadata": {
    "id": "-utZeuGcTsqJ"
   },
   "source": [
    "## Optimizer\n",
    "\n",
    "We used the Adam optimizer [(cite)](https://arxiv.org/abs/1412.6980)\n",
    "with $\\beta_1=0.9$, $\\beta_2=0.98$ and $\\epsilon=10^{-9}$.  We\n",
    "varied the learning rate over the course of training, according to\n",
    "the formula:\n",
    "\n",
    "$$\n",
    "lrate = d_{\\text{model}}^{-0.5} \\cdot\n",
    "  \\min({step\\_num}^{-0.5},\n",
    "    {step\\_num} \\cdot {warmup\\_steps}^{-1.5})\n",
    "$$\n",
    "\n",
    "This corresponds to increasing the learning rate linearly for the\n",
    "first $warmup\\_steps$ training steps, and decreasing it thereafter\n",
    "proportionally to the inverse square root of the step number.  We\n",
    "used $warmup\\_steps=4000$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd2c5c",
   "metadata": {
    "id": "39FbYnt-TsqJ"
   },
   "source": [
    "\n",
    "> Note: This part is very important. Need to train with this setup\n",
    "> of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf8c6a5",
   "metadata": {
    "id": "hlbojFkjTsqJ"
   },
   "source": [
    "\n",
    "> Example of the curves of this model for different model sizes and\n",
    "> for optimization hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbfdd7d",
   "metadata": {
    "id": "zUz3PdAnVg4o"
   },
   "outputs": [],
   "source": [
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    we have to default the step to 1 for LambdaLR function\n",
    "    to avoid zero raising to negative power.\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df913b5-2588-411f-9e8d-4d429ad48c08",
   "metadata": {},
   "source": [
    "这段代码定义了一个函数 `rate`，用于计算学习率调度器的学习率。\n",
    "\n",
    "函数接受以下参数：\n",
    "- `step`：当前的训练步数。\n",
    "- `model_size`：模型的大小。\n",
    "- `factor`：缩放因子，用于调整学习率的比例。\n",
    "- `warmup`：预热步数，用于控制学习率的预热阶段。\n",
    "\n",
    "函数内部执行以下操作：\n",
    "- 首先，对 `step` 进行检查，如果为 0，则将其设置为 1，以避免零的负幂运算。\n",
    "- 计算学习率的公式，其中：\n",
    "  - `model_size ** (-0.5)` 是模型大小的负平方根，用于缩放学习率。\n",
    "  - `min(step ** (-0.5), step * warmup ** (-1.5))` 是两项中的最小值，用于控制学习率在预热阶段和后续阶段的变化。\n",
    "    - 在预热阶段，学习率随着步数的增加以步数的负平方根的速度减小。\n",
    "    - 在后续阶段，学习率随着步数的增加以步数的负平方根的速度减小，但相对于预热阶段，减小的速度较慢。\n",
    "\n",
    "最后，将缩放因子与计算得到的学习率乘积作为最终的学习率返回。\n",
    "\n",
    "这段代码实现了一个基于模型大小、当前步数和预热阶段的学习率计算方法，用于在训练过程中动态调整学习率的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8606b86",
   "metadata": {
    "id": "l1bnrlnSV8J5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def example_learning_schedule():\n",
    "    opts = [\n",
    "        [512, 1, 4000],  # example 1\n",
    "        [512, 1, 8000],  # example 2\n",
    "        [256, 1, 4000],  # example 3\n",
    "    ]\n",
    "\n",
    "    dummy_model = torch.nn.Linear(1, 1)\n",
    "    learning_rates = []\n",
    "\n",
    "    # we have 3 examples in opts list.\n",
    "    for idx, example in enumerate(opts):\n",
    "        # run 20000 epoch for each example\n",
    "        optimizer = torch.optim.Adam(\n",
    "            dummy_model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9\n",
    "        )\n",
    "        lr_scheduler = LambdaLR(\n",
    "            optimizer=optimizer, lr_lambda=lambda step: rate(step, *example)\n",
    "        )\n",
    "        tmp = []\n",
    "        # take 20K dummy training steps, save the learning rate at each step\n",
    "        for step in range(20000):\n",
    "            tmp.append(optimizer.param_groups[0][\"lr\"])\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "        learning_rates.append(tmp)\n",
    "\n",
    "    learning_rates = torch.tensor(learning_rates)\n",
    "\n",
    "    # Enable altair to handle more than 5000 rows\n",
    "    alt.data_transformers.disable_max_rows()\n",
    "\n",
    "    opts_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Learning Rate\": learning_rates[warmup_idx, :],\n",
    "                    \"model_size:warmup\": [\"512:4000\", \"512:8000\", \"256:4000\"][\n",
    "                        warmup_idx\n",
    "                    ],\n",
    "                    \"step\": range(20000),\n",
    "                }\n",
    "            )\n",
    "            for warmup_idx in [0, 1, 2]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(opts_data)\n",
    "        .mark_line()\n",
    "        .properties(width=600)\n",
    "        .encode(x=\"step\", y=\"Learning Rate\", color=\"model_size:warmup:N\")\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "example_learning_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa05991b-4a58-4e3d-98ed-b12b82c8c16a",
   "metadata": {},
   "source": [
    "这段代码展示了一个学习率的示例调度表。它使用了不同的模型大小和预热步数组合来生成学习率的变化曲线，并使用 Altair 可视化库绘制了学习率随训练步数的变化。\n",
    "\n",
    "代码的执行流程如下：\n",
    "- 定义了一个包含 3 个示例的列表 `opts`，每个示例包含模型大小、缩放因子和预热步数。\n",
    "- 创建了一个名为 `dummy_model` 的线性模型实例，用于演示目的。\n",
    "- 定义了一个空列表 `learning_rates`，用于存储不同示例下的学习率变化。\n",
    "- 对于每个示例，进行如下操作：\n",
    "  - 创建一个 Adam 优化器，并设置初始学习率、动量参数和 epsilon。\n",
    "  - 创建一个学习率调度器 `lr_scheduler`，使用 LambdaLR 类，传入优化器和学习率计算函数 `lr_lambda`，其中 `lr_lambda` 调用了前面定义的 `rate` 函数，参数是当前步数和示例中的模型大小、缩放因子和预热步数。\n",
    "  - 创建临时列表 `tmp`，用于存储每个训练步骤的学习率。\n",
    "  - 进行 20000 个虚拟的训练步骤：\n",
    "    - 将当前学习率添加到临时列表 `tmp` 中。\n",
    "    - 执行优化器的参数更新。\n",
    "    - 执行学习率调度器的更新。\n",
    "  - 将临时列表 `tmp` 添加到 `learning_rates` 列表中。\n",
    "- 将 `learning_rates` 转换为 PyTorch 的张量。\n",
    "- 使用 Altair 可视化库，创建一个折线图，展示学习率随训练步数的变化。图表的 x 轴为步数，y 轴为学习率，颜色区分不同模型大小和预热步数的组合。\n",
    "- 返回生成的可视化图表。\n",
    "\n",
    "这段代码的目的是通过可视化展示不同模型大小和预热步数对学习率调度的影响，帮助理解学习率在训练过程中的变化情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec7eac0-d7dd-4d12-b8a5-504b32291965",
   "metadata": {},
   "source": [
    "---\n",
    "`torch.optim.Adam` 和 `torch.optim.SGD` 是 PyTorch 中常用的优化器类，用于在训练神经网络时更新模型的参数。\n",
    "\n",
    "主要的差别如下：\n",
    "\n",
    "1. **更新策略**：\n",
    "   - Adam（Adaptive Moment Estimation）：Adam 是一种自适应学习率优化算法，结合了动量梯度下降和RMSprop的思想。它使用梯度的一阶矩估计（均值）和二阶矩估计（方差）来自适应地调整学习率。\n",
    "   - SGD（Stochastic Gradient Descent）：SGD 是随机梯度下降的简称，它根据每个样本的梯度更新模型参数。SGD 更新的方向是在当前样本的梯度方向上，所以它具有一定的随机性。\n",
    "\n",
    "2. **动量**：\n",
    "   - Adam：Adam 使用动量来加速梯度下降，并在更新时考虑了梯度的一阶矩估计（动量）。\n",
    "   - SGD：SGD 也可以使用动量来加速收敛，通过在更新时引入上一步的更新方向的权重。这有助于克服局部最小值，并在参数空间中更快地搜索。\n",
    "\n",
    "3. **学习率调度**：\n",
    "   - Adam：Adam 使用自适应的学习率调度，根据参数梯度的估计值来自适应地调整学习率。\n",
    "   - SGD：SGD 的学习率通常是预先指定的，并且可以在训练过程中使用学习率调度器进行动态调整。\n",
    "\n",
    "总的来说，Adam 适用于大多数情况，并且在许多任务和网络架构中表现良好。它通常能够更快地收敛，并且对学习率的选择更加鲁棒。然而，对于某些特定的问题和网络，SGD 也可能表现出色，特别是当使用合适的学习率调度策略和动量设置时。在实践中，选择优化器通常需要根据具体问题和实验来进行调整和比较。\n",
    "\n",
    "---\n",
    "`LambdaLR` 是 PyTorch 中的一个学习率调度器类，用于根据自定义的函数调整优化器的学习率。它可以根据训练的当前步数或 epoch 数量来动态地改变学习率。\n",
    "\n",
    "`LambdaLR` 的作用是根据用户定义的 `lr_lambda` 函数对每个优化器的学习率进行调整。该函数接受当前的步数或 epoch 数量作为输入，并返回相应的学习率因子。学习率因子会乘以初始学习率，从而得到调整后的学习率。\n",
    "\n",
    "使用 `LambdaLR` 可以实现各种学习率调度策略，例如：\n",
    "- 线性学习率衰减：随着训练步数或 epoch 的增加，逐渐降低学习率。\n",
    "- 指数学习率衰减：学习率按指数衰减，例如每个 epoch 减少一定的倍数。\n",
    "- 阶梯学习率调整：在特定的步数或 epoch 处更改学习率，例如进行一些预定的学习率衰减或增加操作。\n",
    "\n",
    "使用示例：\n",
    "```python\n",
    "# 创建优化器和模型\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 创建 LambdaLR 调度器，定义学习率调度函数\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: 0.95 ** step)\n",
    "\n",
    "# 在训练循环中更新学习率\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(data_loader):\n",
    "        # 执行优化器的参数更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # 执行学习率调度器的更新\n",
    "        lr_scheduler.step()\n",
    "```\n",
    "\n",
    "通过自定义 `lr_lambda` 函数，可以根据需要灵活地调整学习率。可以根据当前的步数或 epoch 数量制定自己的学习率调度策略，以优化训练过程并改善模型的收敛效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90dd401",
   "metadata": {
    "id": "7T1uD15VTsqK"
   },
   "source": [
    "## Regularization\n",
    "\n",
    "### Label Smoothing\n",
    "\n",
    "During training, we employed label smoothing of value\n",
    "$\\epsilon_{ls}=0.1$ [(cite)](https://arxiv.org/abs/1512.00567).\n",
    "This hurts perplexity, as the model learns to be more unsure, but\n",
    "improves accuracy and BLEU score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f4aa27",
   "metadata": {
    "id": "kNoAVD8bTsqK"
   },
   "source": [
    "\n",
    "> We implement label smoothing using the KL div loss. Instead of\n",
    "> using a one-hot target distribution, we create a distribution that\n",
    "> has `confidence` of the correct word and the rest of the\n",
    "> `smoothing` mass distributed throughout the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06711c2e",
   "metadata": {
    "id": "shU2GyiETsqK",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21830d96-6403-4e37-b6b4-2a802f4fabf2",
   "metadata": {},
   "source": [
    "上述代码实现了标签平滑（Label Smoothing）的功能。\n",
    "\n",
    "标签平滑是一种用于改善分类任务中模型的泛化能力的技术。在传统的分类任务中，标签通常是使用独热编码表示的，即目标类别的索引位置为1，其它位置为0。然而，独热编码的标签在训练过程中可能导致模型过于自信和过拟合。标签平滑通过将一部分置信度分配给非目标类别，来减少模型的过拟合。\n",
    "\n",
    "该代码中的 `LabelSmoothing` 类继承自 `nn.Module`，它的作用是计算带有标签平滑的交叉熵损失。以下是代码的详细解析：\n",
    "\n",
    "- `__init__(self, size, padding_idx, smoothing=0.0)`：初始化函数，接受以下参数：\n",
    "  - `size`：标签的类别数量。\n",
    "  - `padding_idx`：填充索引，用于忽略填充标签的损失计算。\n",
    "  - `smoothing`：标签平滑参数，用于控制非目标类别的置信度。\n",
    "\n",
    "- `forward(self, x, target)`：前向传播函数，接受输入 `x` 和目标标签 `target`，返回计算得到的标签平滑损失。\n",
    "  - `x` 是模型的输出结果，通常是经过 softmax 激活函数处理后的预测概率。\n",
    "  - `target` 是真实的目标标签。\n",
    "\n",
    "在前向传播过程中，首先通过 `x.data.clone()` 创建一个与 `x` 形状相同的张量 `true_dist`，并将所有元素初始化为标签平滑参数除以类别数量减去2（即目标类别和填充类别之外的类别数量）。然后，使用 `scatter_` 方法将真实目标标签的索引位置处设置为标签平滑参数的另一个部分，从而分配置信度给目标类别。同时，将填充类别的置信度设置为0。接下来，通过 `torch.nonzero` 找到目标标签中为填充类别的位置，并将 `true_dist` 中对应的位置置为0。最后，使用 `nn.KLDivLoss` 计算模型预测的概率分布 `x` 和标签平滑后的真实分布 `true_dist` 之间的 KL 散度损失。\n",
    "\n",
    "通过使用 `LabelSmoothing` 类，可以将标签平滑应用于分类任务的损失计算中，从而提升模型的泛化性能。\n",
    "\n",
    "---\n",
    "\n",
    "代码中的`target.data.unsqueeze(1)` 是将 `target.data` 张量在维度1上进行扩展操作。\n",
    "\n",
    "在PyTorch中，`unsqueeze(dim)` 方法用于在指定的维度上增加一个维度。参数 `dim` 指定了要插入的新维度的索引位置。\n",
    "\n",
    "具体到这段代码中，`target.data` 是一个一维的张量，形状为 `[batch_size]`，其中每个元素表示一个样本的目标标签。调用 `unsqueeze(1)` 后，会在维度1上插入一个新的维度，使其形状变为 `[batch_size, 1]`。也就是说，每个目标标签都会变成一个包含单个元素的一维张量。\n",
    "\n",
    "这样做的目的是为了与 `true_dist` 张量进行相应的索引操作，使其能够在正确的位置上分配标签平滑参数。在代码中，`true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)` 使用 `target.data.unsqueeze(1)` 作为索引，在维度1上将 `self.confidence` 的值分配给相应的位置。\n",
    "\n",
    "通过使用 `unsqueeze(1)`，可以确保目标标签的形状与 `true_dist` 张量的形状一致，以便进行正确的索引操作。\n",
    "\n",
    "---\n",
    "\n",
    "`true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)` 是对 `true_dist` 张量进行按索引散射（scatter）操作。\n",
    "\n",
    "在 PyTorch 中，`scatter_(dim, index, src)` 方法用于根据索引 `index` 将源张量 `src` 的值散射到目标张量中的指定维度 `dim` 上的指定位置。下划线（`_`）表示该方法是原地操作，会修改原始张量。\n",
    "\n",
    "具体到这段代码中，`true_dist` 是一个形状为 `[batch_size, num_classes]` 的张量，表示标签平滑后的真实分布。`target.data.unsqueeze(1)` 是形状为 `[batch_size, 1]` 的索引张量，表示每个样本的目标标签。\n",
    "\n",
    "通过调用 `scatter_` 方法，将 `self.confidence` 的值散射到 `true_dist` 张量的维度1上的指定位置，使得每个样本的目标标签处的位置获得更高的置信度。\n",
    "\n",
    "具体操作如下：\n",
    "- `dim=1` 表示在维度1上进行散射操作，即对每个样本的目标标签位置进行操作。\n",
    "- `index=target.data.unsqueeze(1)` 表示使用 `target.data.unsqueeze(1)` 作为索引，指定要散射的位置。\n",
    "- `src=self.confidence` 表示使用 `self.confidence` 的值作为散射的源，即要散射到目标张量的值。\n",
    "\n",
    "综上所述，`true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)` 将 `self.confidence` 的值按照目标标签的索引位置，散射到 `true_dist` 张量的相应位置上，以形成标签平滑后的真实分布。\n",
    "\n",
    "---\n",
    "\n",
    "`nn.KLDivLoss` 是用于计算 KL 散度损失（Kullback-Leibler Divergence Loss）的函数。KL 散度是衡量两个概率分布之间差异的指标。\n",
    "\n",
    "在这段代码中，`nn.KLDivLoss` 的输入是模型预测的概率分布 `x` 和标签平滑后的真实分布 `true_dist`。它会计算这两个分布之间的 KL 散度，并返回对应的损失值。\n",
    "\n",
    "KL 散度衡量了模型预测分布相对于真实分布的差异程度。当模型的预测分布与真实分布完全一致时，KL 散度为0。当两个分布差异较大时，KL 散度的值会较大。\n",
    "\n",
    "在标签平滑的情况下，真实分布进行了平滑处理，以减少模型过于自信或过于绝对的预测。通过计算模型预测分布和平滑后的真实分布之间的 KL 散度损失，可以引导模型更加准确地拟合平滑后的目标分布，从而提高模型的泛化能力和鲁棒性。\n",
    "\n",
    "因此，在这段代码中，`nn.KLDivLoss` 的计算目的是为了最小化模型预测分布与标签平滑后的真实分布之间的差异，从而使模型更好地适应平滑后的目标分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f6413",
   "metadata": {
    "id": "jCxUrlUyTsqK"
   },
   "source": [
    "\n",
    "> Here we can see an example of how the mass is distributed to the\n",
    "> words based on confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54577f1",
   "metadata": {
    "id": "EZtKaaQNTsqK",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Example of label smoothing.\n",
    "\n",
    "\n",
    "def example_label_smoothing():\n",
    "    crit = LabelSmoothing(5, 0, 0.4)\n",
    "    predict = torch.FloatTensor(\n",
    "        [\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "        ]\n",
    "    )\n",
    "    crit(x=predict.log(), target=torch.LongTensor([2, 1, 0, 3, 3]))\n",
    "    print(crit.true_dist)\n",
    "    LS_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"target distribution\": crit.true_dist[x, y].flatten(),\n",
    "                    \"columns\": y,\n",
    "                    \"rows\": x,\n",
    "                }\n",
    "            )\n",
    "            for y in range(5)\n",
    "            for x in range(5)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(LS_data)\n",
    "        .mark_rect(color=\"Blue\", opacity=1)\n",
    "        .properties(height=200, width=200)\n",
    "        .encode(\n",
    "            alt.X(\"columns:O\", title=None),\n",
    "            alt.Y(\"rows:O\", title=None),\n",
    "            alt.Color(\n",
    "                \"target distribution:Q\", scale=alt.Scale(scheme=\"viridis\")\n",
    "            ),\n",
    "        )\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "show_example(example_label_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1550e8e0-5a73-4a22-b37d-eeee9fd781b6",
   "metadata": {},
   "source": [
    "这段代码演示了标签平滑（label smoothing）的例子。\n",
    "\n",
    "首先，定义了一个 `LabelSmoothing` 类，用于实现标签平滑的操作。该类继承自 `nn.Module`，其中的 `forward` 方法实现了标签平滑的计算逻辑。\n",
    "\n",
    "在 `example_label_smoothing` 函数中，首先创建了一个 `LabelSmoothing` 对象 `crit`，指定了类别数为5，填充索引为0，平滑因子为0.4。\n",
    "\n",
    "接下来，创建了一个形状为 `[5, 5]` 的预测张量 `predict`，其中包含了5个样本的预测概率分布。\n",
    "\n",
    "然后，调用 `crit` 对象的 `forward` 方法，传入预测张量的对数概率 `predict.log()` 和目标标签张量 `torch.LongTensor([2, 1, 0, 3, 3])`，进行标签平滑的计算。\n",
    "\n",
    "接着，通过使用 `pd.concat` 组合了多个 DataFrame，生成了一个包含目标分布数据的 DataFrame `LS_data`。每行代表一个样本，每列代表一个类别，目标分布的值表示目标类别的置信度。\n",
    "\n",
    "最后，使用 `alt.Chart` 创建了一个矩形图，以可视化目标分布的结果。不同的颜色表示不同的目标分布值，从深到浅表示分布值的大小。\n",
    "\n",
    "综上所述，该代码展示了标签平滑操作后的目标分布的可视化结果，通过颜色来表示每个位置上的目标分布值的大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78163eb",
   "metadata": {
    "id": "CGM8J1veTsqK"
   },
   "source": [
    "\n",
    "> Label smoothing actually starts to penalize the model if it gets\n",
    "> very confident about a given choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b9f5b9",
   "metadata": {
    "id": "78EHzLP7TsqK"
   },
   "outputs": [],
   "source": [
    "def loss(x, crit):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d]])\n",
    "    return crit(predict.log(), torch.LongTensor([1])).data\n",
    "\n",
    "\n",
    "def penalization_visualization():\n",
    "    crit = LabelSmoothing(5, 0, 0.1)\n",
    "    loss_data = pd.DataFrame(\n",
    "        {\n",
    "            \"Loss\": [loss(x, crit) for x in range(1, 100)],\n",
    "            \"Steps\": list(range(99)),\n",
    "        }\n",
    "    ).astype(\"float\")\n",
    "\n",
    "    return (\n",
    "        alt.Chart(loss_data)\n",
    "        .mark_line()\n",
    "        .properties(width=350)\n",
    "        .encode(\n",
    "            x=\"Steps\",\n",
    "            y=\"Loss\",\n",
    "        )\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "show_example(penalization_visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d6166-88da-447e-be84-4ffa20679e6e",
   "metadata": {},
   "source": [
    "这段代码实现了一个用于可视化惩罚项的函数。首先，定义了一个 `loss` 函数，它接受一个参数 `x` 和一个 `crit`（`LabelSmoothing` 类的实例），并返回模型的损失值。\n",
    "\n",
    "在 `loss` 函数中，通过对输入 `x` 进行一系列计算，生成了一个预测的概率分布 `predict`。然后，将该预测分布和一个标签值 `[1]` 传递给 `crit` 对象进行计算。其中，标签值 `[1]` 表示目标类别的索引。\n",
    "\n",
    "在 `penalization_visualization` 函数中，使用了一个 `LabelSmoothing` 对象 `crit` 和一组不同的 `x` 值，通过调用 `loss` 函数计算了对应的损失值。然后，将损失值和相应的步骤数放入一个数据框 `loss_data` 中。\n",
    "\n",
    "最后，使用 `alt.Chart` 创建一个图表，将步骤数作为 x 轴，损失值作为 y 轴进行可视化。通过这个图表可以观察到，随着输入 `x` 的增加，损失值逐渐增加，表明在使用标签平滑时，模型的预测与目标之间的差异增大，从而引入了一定的惩罚效果。\n",
    "\n",
    "该图表的目的是可视化标签平滑的惩罚效果，帮助理解在不同输入情况下，惩罚项对模型的损失值的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62da0307",
   "metadata": {
    "id": "67lUqeLXTsqK"
   },
   "source": [
    "# A First  Example\n",
    "\n",
    "> We can begin by trying out a simple copy-task. Given a random set\n",
    "> of input symbols from a small vocabulary, the goal is to generate\n",
    "> back those same symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be51ed4",
   "metadata": {
    "id": "jJa-89_pTsqK"
   },
   "source": [
    "## Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc12e90",
   "metadata": {
    "id": "g1aTxeqqTsqK"
   },
   "outputs": [],
   "source": [
    "def data_gen(V, batch_size, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.randint(1, V, size=(batch_size, 10))\n",
    "        data[:, 0] = 1\n",
    "        src = data.requires_grad_(False).clone().detach()\n",
    "        tgt = data.requires_grad_(False).clone().detach()\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8161c3b1-9bfb-4962-b87e-9016188fe9eb",
   "metadata": {},
   "source": [
    "这段代码定义了一个数据生成器函数 `data_gen`，用于生成用于源-目标复制任务的随机数据。\n",
    "\n",
    "函数接受三个参数：\n",
    "- `V`：表示词汇表的大小，数据中随机生成的单词将在范围 `[1, V)` 内取值。\n",
    "- `batch_size`：表示每个批次的样本数量。\n",
    "- `nbatches`：表示生成的批次数量。\n",
    "\n",
    "在生成数据的过程中，每个批次会生成一个大小为 `batch_size` 的随机数据张量 `data`，形状为 `(batch_size, 10)`。其中，数据的每一行表示一个序列，包括 10 个单词。\n",
    "\n",
    "为了模拟源-目标复制任务，将每个序列的第一个单词设置为 1，表示起始标记。这是为了让模型学会复制源序列中的其余单词到目标序列中。\n",
    "\n",
    "然后，使用 `.requires_grad_(False).clone().detach()` 对生成的数据张量进行浅拷贝，并设置 `requires_grad` 属性为 `False`，使得数据张量不参与梯度计算。这是因为在生成数据时，我们只需要输入和目标数据，而不需要计算其梯度。\n",
    "\n",
    "最后，使用 `Batch` 类将源数据 `src` 和目标数据 `tgt` 封装成一个批次对象，并使用 `yield` 语句将该批次返回，从而实现了数据的生成器功能。\n",
    "\n",
    "调用该生成器函数可以获得一系列包含源数据和目标数据的批次对象，用于训练或评估模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08003869",
   "metadata": {
    "id": "XTXwD9hUTsqK"
   },
   "source": [
    "## Loss Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0a756b",
   "metadata": {
    "id": "3J8EJm87TsqK"
   },
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        sloss = (\n",
    "            self.criterion(\n",
    "                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n",
    "            )\n",
    "            / norm\n",
    "        )\n",
    "        return sloss.data * norm, sloss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa46028-430c-4758-96d4-4e95f5414b13",
   "metadata": {},
   "source": [
    "这段代码定义了一个简单的损失计算和训练函数 `SimpleLossCompute`。\n",
    "\n",
    "该函数接受两个参数：\n",
    "- `generator`：一个生成器模型，用于将输入 `x` 转换为预测结果。\n",
    "- `criterion`：损失函数，用于计算预测结果与目标 `y` 之间的损失。\n",
    "\n",
    "函数的 `__call__` 方法定义了损失计算和训练的逻辑。它接受三个参数：\n",
    "- `x`：预测结果，经过生成器模型生成的张量。\n",
    "- `y`：目标标签，表示真实值的张量。\n",
    "- `norm`：归一化系数，用于对损失进行归一化。\n",
    "\n",
    "在函数内部，首先将预测结果 `x` 输入到生成器模型中，得到最终的预测值。然后，通过调用损失函数 `criterion` 计算预测结果 `x` 和目标标签 `y` 之间的损失。注意，为了方便计算，需要对 `x` 和 `y` 进行形状调整，将其转换为二维张量。最后，将损失除以归一化系数 `norm` 进行归一化。\n",
    "\n",
    "函数返回两个值：\n",
    "- `sloss.data * norm`：未经归一化的损失值，乘以 `norm` 后表示实际损失。\n",
    "- `sloss`：归一化后的损失值。\n",
    "\n",
    "该函数主要用于简化损失计算和训练过程的代码，使代码更加清晰和易读。可以根据实际需求进行修改和扩展。\n",
    "\n",
    "---\n",
    "\n",
    "`x.contiguous().view(-1, x.size(-1))` 的含义是将张量 `x` 进行连续化操作（contiguous）并改变其形状。\n",
    "\n",
    "在PyTorch中，有些操作要求输入张量是连续的（contiguous），即内存中元素的存储是连续的。而有些操作可能会导致张量的存储变得不连续，这时需要使用 `contiguous` 方法将其转为连续的存储。\n",
    "\n",
    "`.view(-1, x.size(-1))` 的作用是将张量 `x` 的形状调整为 `(batch_size * seq_len, num_classes)`，其中 `batch_size` 表示批量大小，`seq_len` 表示序列长度，`num_classes` 表示类别数量。这个操作会保持张量中元素的顺序不变，只是将张量进行重新排列，以便进行后续的损失计算。\n",
    "\n",
    "总而言之，`x.contiguous().view(-1, x.size(-1))` 将张量 `x` 进行连续化操作，并将其形状调整为二维张量，方便进行损失计算和其他操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e2191",
   "metadata": {
    "id": "eDAI7ELUTsqL"
   },
   "source": [
    "## Greedy Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec73684",
   "metadata": {
    "id": "LFkWakplTsqL",
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "> This code predicts a translation using greedy decoding for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b20b9e",
   "metadata": {
    "id": "N2UOpnT3bIyU",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len - 1):\n",
    "        out = model.decode(\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac49458-5b93-4ef4-88fd-9770884a971f",
   "metadata": {},
   "source": [
    "这段代码实现了贪婪解码（Greedy Decoding）的过程，用于生成模型的预测序列。\n",
    "\n",
    "输入参数包括：\n",
    "- `model`：要使用的模型\n",
    "- `src`：输入序列\n",
    "- `src_mask`：输入序列的掩码\n",
    "- `max_len`：生成序列的最大长度\n",
    "- `start_symbol`：起始符号的索引\n",
    "\n",
    "函数首先通过模型的编码器将输入序列编码为一个表示记忆（memory）。然后，初始化输出序列 `ys`，只包含起始符号。接下来，通过循环逐步生成序列的每个元素，直到达到最大长度。在每个时间步，模型的解码器根据记忆、输入序列的掩码和当前生成的序列 `ys` 预测下一个时间步的输出。通过模型的生成器（generator），将解码器输出的最后一个时间步的结果转换为概率分布。然后，选择概率最高的单词作为下一个时间步的输出，并将其添加到输出序列 `ys` 中。循环直到生成序列达到最大长度为止。\n",
    "\n",
    "最终，函数返回生成的序列 `ys`。\n",
    "\n",
    "总结来说，`greedy_decode` 函数利用 Transformer 模型对输入序列进行贪婪解码，生成预测序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd4bf3",
   "metadata": {
    "id": "qgIZ2yEtdYwe",
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the simple copy task.\n",
    "def example_simple_model():\n",
    "    V = 11\n",
    "    criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "    model = make_model(V, V, N=2)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, model_size=model.src_embed[0].d_model, factor=1.0, warmup=400\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    batch_size = 80\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        run_epoch(\n",
    "            data_gen(V, batch_size, 20),\n",
    "            model,\n",
    "            SimpleLossCompute(model.generator, criterion),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train\",\n",
    "        )\n",
    "        model.eval()\n",
    "        run_epoch(\n",
    "            data_gen(V, batch_size, 5),\n",
    "            model,\n",
    "            SimpleLossCompute(model.generator, criterion),\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            mode=\"eval\",\n",
    "        )[0]\n",
    "\n",
    "    model.eval()\n",
    "    src = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "    max_len = src.shape[1]\n",
    "    src_mask = torch.ones(1, 1, max_len)\n",
    "    print(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=0))\n",
    "\n",
    "\n",
    "#execute_example(example_simple_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00db2913-8e95-421a-8b13-f099ea49028d",
   "metadata": {},
   "source": [
    "这段代码演示了如何训练一个简单的复制任务（copy task）并使用训练好的模型进行预测。\n",
    "\n",
    "首先，定义了一个大小为 11 的标签平滑（Label Smoothing）损失函数 `criterion`，和一个由 `make_model` 创建的 Transformer 模型 `model`。然后，定义了优化器 `optimizer` 和学习率调度器 `lr_scheduler`。\n",
    "\n",
    "接下来，进入训练循环，共进行 20 个周期（epochs）。在每个周期中，模型分为训练模式和评估模式。调用 `run_epoch` 函数对训练数据进行训练或评估，并传入相应的损失计算对象、优化器和学习率调度器。训练模式下使用真实的优化器和学习率调度器进行参数更新，评估模式下使用虚拟的优化器和学习率调度器仅进行损失计算。训练模式中，每个周期的训练数据使用 `data_gen` 生成。\n",
    "\n",
    "训练完成后，将模型切换为评估模式，并使用 `greedy_decode` 函数对输入序列进行贪婪解码，生成预测序列。\n",
    "\n",
    "最终，打印输出预测序列。\n",
    "\n",
    "总结来说，这段代码展示了如何使用 Transformer 模型训练和预测一个简单的复制任务，并展示了训练过程中的损失和预测结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3781e888",
   "metadata": {
    "id": "OpuQv2GsTsqL"
   },
   "source": [
    "# Part 3: A Real World Example\n",
    "\n",
    "> Now we consider a real-world example using the Multi30k\n",
    "> German-English Translation task. This task is much smaller than\n",
    "> the WMT task considered in the paper, but it illustrates the whole\n",
    "> system. We also show how to use multi-gpu processing to make it\n",
    "> really fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c8b7cd",
   "metadata": {
    "id": "8y9dpfolTsqL",
    "tags": []
   },
   "source": [
    "## Data Loading\n",
    "\n",
    "> We will load the dataset using torchtext and spacy for\n",
    "> tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy tokenizer models, download them if they haven't been\n",
    "# downloaded already\n",
    "\n",
    "\n",
    "def load_tokenizers():\n",
    "\n",
    "    try:\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download de_core_news_sm\")\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    try:\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download en_core_web_sm\")\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    return spacy_de, spacy_en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795eab3-a3b9-4a8c-aa4c-a879d1094ec7",
   "metadata": {},
   "source": [
    "这段代码用于加载 spaCy 分词器模型。如果模型尚未下载，它将自动下载所需的模型。\n",
    "\n",
    "首先，尝试加载 \"de_core_news_sm\" 的 spaCy 分词器模型。如果模型加载失败（即抛出 IOError 异常），则通过执行命令 `python -m spacy download de_core_news_sm` 下载并安装所需的模型，然后再次尝试加载模型。\n",
    "\n",
    "接下来，尝试加载 \"en_core_web_sm\" 的 spaCy 分词器模型。如果模型加载失败，同样执行命令 `python -m spacy download en_core_web_sm` 下载并安装所需的模型，然后再次尝试加载模型。\n",
    "\n",
    "最后，返回加载成功的 spaCy 分词器模型对象 `spacy_de` 和 `spacy_en`。\n",
    "\n",
    "这段代码确保在使用 spaCy 分词器之前，必要的模型已经下载和加载。\n",
    "\n",
    "---\n",
    "\n",
    "变量 `spacy_de` 和 `spacy_en` 是 spaCy 分词器模型对象。它们是经过训练和加载的自然语言处理模型，可以用于对德语（`spacy_de`）和英语（`spacy_en`）进行分词操作。\n",
    "\n",
    "这些对象具有各种方法和属性，可以用于执行文本处理任务，如分词、词性标注、命名实体识别等。通过调用这些对象的方法，可以将文本输入模型，然后获得相应的分词结果或其他文本处理结果。\n",
    "\n",
    "例如，可以使用以下方式调用分词器模型对象进行分词操作：\n",
    "\n",
    "```python\n",
    "text = \"This is an example sentence.\"\n",
    "tokens = spacy_en(text)\n",
    "```\n",
    "\n",
    "在上述示例中，`spacy_en` 是加载的英语分词器模型对象，`text` 是待分词的文本。通过调用 `spacy_en` 对象，将文本传递给它，就可以获得对应的分词结果 `tokens`。\n",
    "\n",
    "需要注意的是，这里的 `spacy_de` 和 `spacy_en` 是根据代码中的变量命名推测的对象名称，实际命名可以根据代码的上下文而有所不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eebff2",
   "metadata": {
    "id": "t4BszXXJTsqL",
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(text, tokenizer):\n",
    "    return [tok.text for tok in tokenizer.tokenizer(text)]\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter, tokenizer, index):\n",
    "    for from_to_tuple in data_iter:\n",
    "        yield tokenizer(from_to_tuple[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeafc876-3c0f-4cc9-ab45-10f6b7096d0b",
   "metadata": {},
   "source": [
    "这段代码包含两个函数：`tokenize` 和 `yield_tokens`。\n",
    "\n",
    "1. `tokenize(text, tokenizer)` 函数接受一个文本字符串 `text` 和一个分词器 `tokenizer` 作为输入，并返回该文本的分词结果。它通过遍历 `tokenizer.tokenizer(text)` 得到的分词对象列表，提取每个分词对象的 `text` 属性，将其作为分词结果返回。\n",
    "\n",
    "2. `yield_tokens(data_iter, tokenizer, index)` 函数是一个生成器函数，它接受一个数据迭代器 `data_iter`、一个分词器 `tokenizer` 和一个索引 `index` 作为输入。它通过遍历 `data_iter` 中的元素（假设每个元素是一个包含多个字段的元组），获取索引 `index` 对应的字段，然后将该字段的文本输入到分词器 `tokenizer` 中进行分词。生成器函数使用 `yield` 关键字产生每个分词结果，使得可以逐步获取分词结果而不需要一次性加载全部数据。\n",
    "\n",
    "这两个函数可以结合使用，例如：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d7b1e-e447-439b-84ba-2dd1f9359f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de, spacy_en = load_tokenizers()\n",
    "\n",
    "text = \"This is an example sentence.\"\n",
    "tokens = tokenize(text, spacy_en)\n",
    "print(tokens)\n",
    "data_iter = [(\"Sentence 1\", \"Sentence 2\"), (\"Sentence 3\", \"Sentence 4\")]\n",
    "token_generator = yield_tokens(data_iter, spacy_en, index=0)\n",
    "for tokens in token_generator:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb90ae-7852-47a2-a442-44bfbb05a318",
   "metadata": {},
   "source": [
    "在上述示例中，我们首先加载了英语分词器模型 `spacy_en`，然后使用 `tokenize` 函数将文本进行分词。接下来，我们定义了一个数据迭代器 `data_iter`，其中包含了一些元组数据。通过调用 `yield_tokens` 函数生成一个分词器的生成器，并逐步获取分词结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648fe9c0",
   "metadata": {
    "id": "jU3kVlV5okC-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(spacy_de, spacy_en):\n",
    "    def tokenize_de(text):\n",
    "        return tokenize(text, spacy_de)\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return tokenize(text, spacy_en)\n",
    "\n",
    "    print(\"Building German Vocabulary ...\")\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    vocab_src = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val + test, tokenize_de, index=0),\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    print(\"Building English Vocabulary ...\")\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    vocab_tgt = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val + test, tokenize_en, index=1),\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    vocab_src.set_default_index(vocab_src[\"<unk>\"])\n",
    "    vocab_tgt.set_default_index(vocab_tgt[\"<unk>\"])\n",
    "\n",
    "    return vocab_src, vocab_tgt\n",
    "\n",
    "\n",
    "def load_vocab(spacy_de, spacy_en):\n",
    "    if not exists(\"vocab.pt\"):\n",
    "        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)\n",
    "        torch.save((vocab_src, vocab_tgt), \"vocab.pt\")\n",
    "    else:\n",
    "        vocab_src, vocab_tgt = torch.load(\"vocab.pt\")\n",
    "    print(\"Finished.\\nVocabulary sizes:\")\n",
    "    print(len(vocab_src))\n",
    "    print(len(vocab_tgt))\n",
    "    return vocab_src, vocab_tgt\n",
    "\n",
    "\n",
    "if is_interactive_notebook():\n",
    "    # global variables used later in the script\n",
    "    spacy_de, spacy_en = show_example(load_tokenizers)\n",
    "    vocab_src, vocab_tgt = show_example(load_vocab, args=[spacy_de, spacy_en])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393dbcb0-85f8-41d6-8999-f8404877783d",
   "metadata": {},
   "source": [
    "这段代码包含两个函数：`build_vocabulary` 和 `load_vocab`，以及一些全局变量的设置。\n",
    "\n",
    "1. `build_vocabulary(spacy_de, spacy_en)` 函数用于构建德语和英语的词汇表。它首先定义了两个内部函数 `tokenize_de` 和 `tokenize_en`，分别用于对德语和英语文本进行分词。接下来，它使用 `datasets.Multi30k` 函数加载 Multi30k 数据集的训练集、验证集和测试集。然后，分别针对德语和英语文本，调用 `yield_tokens` 函数生成分词器的生成器，并通过 `build_vocab_from_iterator` 函数构建词汇表。该函数还设置了一些特殊的标记（`<s>`、`</s>`、`<blank>`、`<unk>`）以及最小频率参数。最后，将默认索引设置为 `<unk>` 标记，并返回德语和英语的词汇表对象。\n",
    "\n",
    "2. `load_vocab(spacy_de, spacy_en)` 函数用于加载词汇表。它首先检查是否已存在名为 \"vocab.pt\" 的文件，如果不存在，则调用 `build_vocabulary` 函数构建词汇表，并将结果保存到 \"vocab.pt\" 文件中。如果文件已存在，则直接加载词汇表。最后，打印词汇表的大小，并返回德语和英语的词汇表对象。\n",
    "\n",
    "在代码的最后部分，如果当前环境是交互式笔记本，则展示了 `load_tokenizers` 和 `load_vocab` 函数的示例运行，并将结果赋值给全局变量 `spacy_de` 和 `spacy_en`、`vocab_src` 和 `vocab_tgt`。\n",
    "\n",
    "这些函数的目的是准备语言模型所需的分词器和词汇表。分词器用于将文本拆分为单词或子词，而词汇表包含了模型所需的单词或子词的索引和相关信息。\n",
    "\n",
    "---\n",
    "\n",
    "**关于 Multi30K**\n",
    "\n",
    "Multi30k 是一个用于多语言机器翻译任务的数据集。它是在 WMT 2016 评测活动中发布的，用于英语到德语的翻译任务。该数据集源自于 Flickr 图像共享平台上的图像描述任务，每个图像都有多个对应的英语和德语描述。\n",
    "\n",
    "Multi30k 数据集包含了约 3 万个图像，共计约 29,000 个训练样本、1,014 个验证样本和 1,000 个测试样本。图像和对应的描述被组织成多个标签文件，每个文件都包含了图像的 ID、英语描述和德语描述。\n",
    "\n",
    "该数据集的主要目标是让模型学习从图像到对应语言的翻译关系。因此，对于机器翻译任务，我们可以将图像描述作为源语言输入，将目标语言描述作为目标输出，训练模型来完成翻译任务。\n",
    "\n",
    "Multi30k 数据集是自然语言处理和机器翻译领域常用的数据集之一，用于评估不同模型在跨语言翻译任务上的性能。Multi30k 数据集包含来自多个来源的英语-德语平行句子，用于图像描述任务。每个样本包含一张图像和对应的英语和德语描述。这些描述涵盖了各种主题和语言风格。该数据集广泛用于机器翻译和图像字幕生成等自然语言处理任务的研究和评估\n",
    "\n",
    "---\n",
    "**关于 vocab.pt**\n",
    "\n",
    "`vocab.pt` 是一个保存了源语言和目标语言的词汇表的文件。它使用 PyTorch 的 `torch.save()` 函数进行保存。保存的文件格式是 PyTorch 的序列化格式，其中包含了两个对象：`vocab_src` 和 `vocab_tgt`，分别表示源语言和目标语言的词汇表。\n",
    "\n",
    "你可以使用 `torch.load()` 函数加载 `vocab.pt` 文件，并获得源语言和目标语言的词汇表对象。\n",
    "\n",
    "---\n",
    "**关于 词汇表**\n",
    "\n",
    "`vocab_src` 和 `vocab_tgt` 是两个词汇表对象，它们都是 `torchtext.vocab.Vocab` 类的实例。这个类表示一个词汇表，包含了单词到索引的映射以及相关的属性和方法。\n",
    "\n",
    "词汇表对象包含以下主要属性和方法：\n",
    "- `stoi`：单词到索引的字典，可以通过单词来获取其对应的索引。\n",
    "- `itos`：索引到单词的列表，可以通过索引来获取对应的单词。\n",
    "- `freqs`：单词的频率统计信息，包含每个单词在数据集中出现的次数。\n",
    "- `unk_index`：表示未知单词的索引。\n",
    "- `pad_index`：表示填充单词的索引。\n",
    "- `bos_index`：表示序列起始的特殊标记（如`<s>`）的索引。\n",
    "- `eos_index`：表示序列结束的特殊标记（如`</s>`）的索引。\n",
    "\n",
    "可以使用这些属性和方法来对词汇表进行查询和操作，例如查找单词的索引、查找索引对应的单词、获取词汇表的大小等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359807d0",
   "metadata": {
    "id": "-l-TFwzfTsqL"
   },
   "source": [
    "\n",
    "> Batching matters a ton for speed. We want to have very evenly\n",
    "> divided batches, with absolutely minimal padding. To do this we\n",
    "> have to hack a bit around the default torchtext batching. This\n",
    "> code patches their default batching to make sure we search over\n",
    "> enough sentences to find tight batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c1ab3",
   "metadata": {
    "incorrectly_encoded_metadata": "id=\"kDEj-hCgokC-\" tags=[] jp-MarkdownHeadingCollapsed=true"
   },
   "source": [
    "## Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc3ef7f",
   "metadata": {
    "id": "wGsIHFgOokC_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_batch(\n",
    "    batch,\n",
    "    src_pipeline,\n",
    "    tgt_pipeline,\n",
    "    src_vocab,\n",
    "    tgt_vocab,\n",
    "    device,\n",
    "    max_padding=128,\n",
    "    pad_id=2,\n",
    "):\n",
    "    bs_id = torch.tensor([0], device=device)  # <s> token id\n",
    "    eos_id = torch.tensor([1], device=device)  # </s> token id\n",
    "    src_list, tgt_list = [], []\n",
    "    for (_src, _tgt) in batch:\n",
    "        processed_src = torch.cat(\n",
    "            [\n",
    "                bs_id,\n",
    "                torch.tensor(\n",
    "                    src_vocab(src_pipeline(_src)),\n",
    "                    dtype=torch.int64,\n",
    "                    device=device,\n",
    "                ),\n",
    "                eos_id,\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "        processed_tgt = torch.cat(\n",
    "            [\n",
    "                bs_id,\n",
    "                torch.tensor(\n",
    "                    tgt_vocab(tgt_pipeline(_tgt)),\n",
    "                    dtype=torch.int64,\n",
    "                    device=device,\n",
    "                ),\n",
    "                eos_id,\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "        src_list.append(\n",
    "            # warning - overwrites values for negative values of padding - len\n",
    "            pad(\n",
    "                processed_src,\n",
    "                (\n",
    "                    0,\n",
    "                    max_padding - len(processed_src),\n",
    "                ),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "        tgt_list.append(\n",
    "            pad(\n",
    "                processed_tgt,\n",
    "                (0, max_padding - len(processed_tgt)),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    src = torch.stack(src_list)\n",
    "    tgt = torch.stack(tgt_list)\n",
    "    return (src, tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ac055-08b0-428f-a723-8edb3aea4157",
   "metadata": {},
   "source": [
    "这段代码实现了一个`collate_batch`函数，用于将一个批次的数据进行处理和填充。\n",
    "\n",
    "函数接受以下参数：\n",
    "- `batch`：一个批次的数据，其中每个元素是一个源语言句子和目标语言句子的元组。\n",
    "- `src_pipeline`：源语言数据的处理管道函数，用于对源语言句子进行预处理操作。\n",
    "- `tgt_pipeline`：目标语言数据的处理管道函数，用于对目标语言句子进行预处理操作。\n",
    "- `src_vocab`：源语言词汇表对象，用于将源语言句子转换为索引序列。\n",
    "- `tgt_vocab`：目标语言词汇表对象，用于将目标语言句子转换为索引序列。\n",
    "- `device`：指定数据存储的设备（如CPU或GPU）。\n",
    "- `max_padding`：填充后的序列最大长度。\n",
    "- `pad_id`：填充标记的索引。\n",
    "\n",
    "函数的主要步骤如下：\n",
    "1. 对于批次中的每个源语言句子和目标语言句子：\n",
    "   - 使用相应的处理管道函数进行预处理，将句子转换为索引序列。\n",
    "   - 添加起始标记和结束标记，并将其与源语言词汇表和目标语言词汇表转换为Tensor对象。\n",
    "   - 将处理后的序列进行填充，使其达到指定的最大长度。\n",
    "2. 将填充后的源语言序列和目标语言序列转换为Tensor对象，并将它们堆叠成一个Tensor张量作为返回值。\n",
    "\n",
    "该函数的作用是将批次的原始文本数据转换为填充后的Tensor数据，以便输入到模型进行训练或推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57603ccf",
   "metadata": {
    "id": "ka2Ce_WIokC_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    device,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    spacy_de,\n",
    "    spacy_en,\n",
    "    batch_size=12000,\n",
    "    max_padding=128,\n",
    "    is_distributed=True,\n",
    "):\n",
    "    # def create_dataloaders(batch_size=12000):\n",
    "    def tokenize_de(text):\n",
    "        return tokenize(text, spacy_de)\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return tokenize(text, spacy_en)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return collate_batch(\n",
    "            batch,\n",
    "            tokenize_de,\n",
    "            tokenize_en,\n",
    "            vocab_src,\n",
    "            vocab_tgt,\n",
    "            device,\n",
    "            max_padding=max_padding,\n",
    "            pad_id=vocab_src.get_stoi()[\"<blank>\"],\n",
    "        )\n",
    "\n",
    "    train_iter, valid_iter, test_iter = datasets.Multi30k(\n",
    "        language_pair=(\"de\", \"en\")\n",
    "    )\n",
    "\n",
    "    train_iter_map = to_map_style_dataset(\n",
    "        train_iter\n",
    "    )  # DistributedSampler needs a dataset len()\n",
    "    train_sampler = (\n",
    "        DistributedSampler(train_iter_map) if is_distributed else None\n",
    "    )\n",
    "    valid_iter_map = to_map_style_dataset(valid_iter)\n",
    "    valid_sampler = (\n",
    "        DistributedSampler(valid_iter_map) if is_distributed else None\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_iter_map,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(train_sampler is None),\n",
    "        sampler=train_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_iter_map,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(valid_sampler is None),\n",
    "        sampler=valid_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef9acd-9e80-46ef-a7fb-e35331d02ba3",
   "metadata": {},
   "source": [
    "这段代码实现了一个`create_dataloaders`函数，用于创建数据加载器（`DataLoader`）对象，用于批量加载和处理训练、验证和测试数据。\n",
    "\n",
    "函数接受以下参数：\n",
    "- `device`：指定数据存储的设备（如CPU或GPU）。\n",
    "- `vocab_src`：源语言词汇表对象。\n",
    "- `vocab_tgt`：目标语言词汇表对象。\n",
    "- `spacy_de`：用于德语文本分词的SpaCy模型对象。\n",
    "- `spacy_en`：用于英语文本分词的SpaCy模型对象。\n",
    "- `batch_size`：批次大小，默认为12000。\n",
    "- `max_padding`：填充后的序列最大长度，默认为128。\n",
    "- `is_distributed`：是否使用分布式训练，默认为True。\n",
    "\n",
    "函数的主要步骤如下：\n",
    "1. 定义用于将德语文本分词的`tokenize_de`函数和将英语文本分词的`tokenize_en`函数。\n",
    "2. 定义`collate_fn`函数，用于将一个批次的原始文本数据转换为填充后的Tensor数据。\n",
    "3. 使用`datasets.Multi30k`加载训练、验证和测试数据集，并将其转换为Map-style的数据集（`to_map_style_dataset`函数将Iterable-style的数据集转换为Map-style的数据集，需要传入一个具有`__getitem__`方法的对象）。\n",
    "4. 根据是否使用分布式训练，创建训练数据加载器（`train_dataloader`）和验证数据加载器（`valid_dataloader`），并指定批次大小、是否洗牌和采样器（如果使用分布式训练）以及数据的处理函数（`collate_fn`）。\n",
    "5. 返回训练数据加载器和验证数据加载器。\n",
    "\n",
    "该函数的作用是将原始文本数据加载为经过处理和填充的Tensor数据，并创建对应的数据加载器，方便在训练过程中以批次的形式输入到模型中。\n",
    "\n",
    "---\n",
    "\n",
    "**关于to_map_style_dataset**\n",
    "\n",
    "`to_map_style_dataset(train_iter)` 的作用是将原始的迭代式数据集对象 `train_iter` 转换为 Map-style 数据集对象 `train_iter_map`。\n",
    "\n",
    "在 PyTorch 中，存在两种类型的数据集对象：Iterable-style 数据集和 Map-style 数据集。\n",
    "\n",
    "- Iterable-style 数据集：这种数据集对象是基于迭代器的，通过迭代访问数据集中的样本。它没有固定的长度，每次迭代返回一个样本。\n",
    "- Map-style 数据集：这种数据集对象是基于索引的，可以通过索引或键来访问数据集中的样本。它具有固定的长度，并且可以直接通过索引访问指定位置的样本。\n",
    "\n",
    "`to_map_style_dataset` 函数的作用就是将 Iterable-style 数据集对象转换为 Map-style 数据集对象。这样做的目的是为了能够使用 `DataLoader` 加载器更方便地处理数据集，因为 `DataLoader` 通常要求输入的数据集对象是 Map-style 的。\n",
    "\n",
    "在给定的代码中，`train_iter` 是 Multi30k 数据集的 Iterable-style 数据集对象。通过调用 `to_map_style_dataset(train_iter)` 将其转换为 Map-style 数据集对象 `train_iter_map`，以便后续能够使用 `DataLoader` 来加载、迭代和处理训练数据集。\n",
    "\n",
    "--- \n",
    "\n",
    "**关于DataLoader**\n",
    "`DataLoader`是PyTorch中的一个实用类，用于将数据集封装成可迭代的批量加载器。它在训练过程中提供了数据的批量加载、数据打乱、并行加速等功能，简化了数据处理的流程。\n",
    "\n",
    "调用`DataLoader`的作用是将数据集划分为小批次并提供对数据的迭代访问。具体而言，`DataLoader`的主要作用包括：\n",
    "\n",
    "1. 数据加载：将原始数据集加载到内存中，并对数据进行处理和预处理（如分词、填充、编码等）。\n",
    "2. 批次划分：将数据划分为小批次，每个批次包含一定数量的样本。\n",
    "3. 数据打乱：可选择在每个训练周期之前对数据进行打乱，以增加模型的训练稳定性和泛化能力。\n",
    "4. 并行加速：通过使用多个工作线程（`num_workers`参数）并行加载数据，加快数据加载速度，提高训练效率。\n",
    "5. 数据预取：在训练过程中，可以异步预取下一个批次的数据，使模型的训练过程更加流畅。\n",
    "\n",
    "通过调用`DataLoader`，我们可以轻松地对训练、验证和测试数据进行批量加载和处理，从而更高效地训练和评估模型。\n",
    "\n",
    "`train_dataloader = DataLoader(...)` 主要目的是创建一个用于训练的数据加载器（`train_dataloader`），它将训练数据集划分为小批次，并提供对数据的迭代访问。\n",
    "\n",
    "具体而言，上述代码的参数和设置含义如下：\n",
    "\n",
    "- `train_iter_map`: 训练数据集，已经通过`to_map_style_dataset`转换为Map-style数据集，即可通过索引访问样本。\n",
    "- `batch_size`: 每个批次的样本数量。\n",
    "- `shuffle`: 是否在每个训练周期之前对数据进行打乱。如果`train_sampler`为`None`，则默认为`True`，即每个周期都对数据进行打乱。\n",
    "- `sampler`: 数据采样器，用于定义样本的抽样策略。在分布式训练中，使用`DistributedSampler`对数据进行分布式抽样，确保每个进程获取不同的样本。\n",
    "- `collate_fn`: 用于对每个批次中的样本进行处理和预处理的函数。在这里，使用`collate_batch`函数对样本进行填充、编码等操作。\n",
    "\n",
    "通过使用`DataLoader`创建训练数据加载器，可以方便地迭代访问训练数据集的小批次样本，实现高效的模型训练。`DataLoader`会根据设定的参数和设置自动处理数据的加载、打乱和批次划分，简化了数据处理的过程，提高了训练效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc79ba2",
   "metadata": {
    "id": "90qM8RzCTsqM"
   },
   "source": [
    "## Training the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edce9f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_worker(\n",
    "    gpu,\n",
    "    ngpus_per_node,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    spacy_de,\n",
    "    spacy_en,\n",
    "    config,\n",
    "    is_distributed=False,\n",
    "):\n",
    "    print(f\"Train worker process using GPU: {gpu} for training\", flush=True)\n",
    "    torch.cuda.set_device(gpu)\n",
    "\n",
    "    pad_idx = vocab_tgt[\"<blank>\"]\n",
    "    d_model = 512\n",
    "    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n",
    "    model.cuda(gpu)\n",
    "    module = model\n",
    "    is_main_process = True\n",
    "    if is_distributed:\n",
    "        dist.init_process_group(\n",
    "            \"nccl\", init_method=\"env://\", rank=gpu, world_size=ngpus_per_node\n",
    "        )\n",
    "        model = DDP(model, device_ids=[gpu])\n",
    "        module = model.module\n",
    "        is_main_process = gpu == 0\n",
    "\n",
    "    criterion = LabelSmoothing(\n",
    "        size=len(vocab_tgt), padding_idx=pad_idx, smoothing=0.1\n",
    "    )\n",
    "    criterion.cuda(gpu)\n",
    "\n",
    "    train_dataloader, valid_dataloader = create_dataloaders(\n",
    "        gpu,\n",
    "        vocab_src,\n",
    "        vocab_tgt,\n",
    "        spacy_de,\n",
    "        spacy_en,\n",
    "        batch_size=config[\"batch_size\"] // ngpus_per_node,\n",
    "        max_padding=config[\"max_padding\"],\n",
    "        is_distributed=is_distributed,\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, d_model, factor=1, warmup=config[\"warmup\"]\n",
    "        ),\n",
    "    )\n",
    "    train_state = TrainState()\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        if is_distributed:\n",
    "            train_dataloader.sampler.set_epoch(epoch)\n",
    "            valid_dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "        model.train()\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Training ====\", flush=True)\n",
    "        _, train_state = run_epoch(\n",
    "            (Batch(b[0], b[1], pad_idx) for b in train_dataloader),\n",
    "            model,\n",
    "            SimpleLossCompute(module.generator, criterion),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train+log\",\n",
    "            accum_iter=config[\"accum_iter\"],\n",
    "            train_state=train_state,\n",
    "        )\n",
    "\n",
    "        GPUtil.showUtilization()\n",
    "        if is_main_process:\n",
    "            file_path = \"%s%.2d.pt\" % (config[\"file_prefix\"], epoch)\n",
    "            torch.save(module.state_dict(), file_path)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Validation ====\", flush=True)\n",
    "        model.eval()\n",
    "        sloss = run_epoch(\n",
    "            (Batch(b[0], b[1], pad_idx) for b in valid_dataloader),\n",
    "            model,\n",
    "            SimpleLossCompute(module.generator, criterion),\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            mode=\"eval\",\n",
    "        )\n",
    "        print(sloss)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if is_main_process:\n",
    "        file_path = \"%sfinal.pt\" % config[\"file_prefix\"]\n",
    "        torch.save(module.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5cd43f-5a7d-430a-a93e-1abb7384d5a2",
   "metadata": {},
   "source": [
    "这段代码是一个用于训练的工作进程函数 `train_worker()`。它接收一些参数，并执行训练过程。\n",
    "\n",
    "首先，它设置了当前 GPU 设备，并将模型移动到该设备上。如果启用了分布式训练，还会使用 `torch.distributed.init_process_group()` 初始化分布式进程组，并使用 `torch.nn.parallel.DistributedDataParallel()` 将模型包装为分布式数据并行模型。\n",
    "\n",
    "接下来，它创建了损失函数 `criterion`，使用了 `LabelSmoothing` 类定义的标签平滑损失函数，并将其移动到 GPU 设备上。\n",
    "\n",
    "然后，它调用 `create_dataloaders()` 函数创建训练集和验证集的数据加载器。这些数据加载器负责将数据集分成批次，并在训练过程中提供批次数据供模型使用。\n",
    "\n",
    "接着，它创建了优化器 `optimizer`，这里使用了 Adam 优化器，并设置了学习率、动量等参数。\n",
    "\n",
    "然后，它创建了学习率调度器 `lr_scheduler`，使用 `LambdaLR` 类定义的学习率调度器，根据预定义的学习率调度函数进行学习率的更新。\n",
    "\n",
    "接下来，它定义了训练状态 `train_state`，用于跟踪训练过程中的步数、样本数和标记数。\n",
    "\n",
    "接下来，它开始进行训练的循环，循环遍历预定义的训练轮数 `config[\"num_epochs\"]`。\n",
    "\n",
    "在每个训练轮次中，首先设置数据加载器的 epoch，以确保每个 GPU 都使用相同的数据。然后将模型设置为训练模式，并调用 `run_epoch()` 函数执行一个训练周期的训练过程。\n",
    "\n",
    "在训练过程中，它打印训练信息，并保存模型的参数到文件中。然后清空 GPU 缓存，释放不再需要的内存。\n",
    "\n",
    "接着，它将模型设置为评估模式，并调用 `run_epoch()` 函数执行一个训练周期的评估过程，计算验证集上的损失。\n",
    "\n",
    "最后，在主进程中保存最终的模型参数到文件中。\n",
    "\n",
    "整个训练过程中，会根据配置的设定进行分布式训练和 GPU 之间的通信与同步。\n",
    "\n",
    "该代码段是一个多 GPU 训练的示例，每个 GPU 设备都运行一个工作进程，共同参与模型的训练和参数更新，以提高训练效率和速度。\n",
    "\n",
    "---\n",
    "\n",
    "**关于 model.train()**\n",
    "\n",
    "语句 `model.train()` 的作用是将模型设置为训练模式。在训练模式下，模型会启用一些特定的行为，如启用 Batch Normalization 和 Dropout 层的计算，并记录梯度以进行参数更新。\n",
    "\n",
    "在训练模式下，模型的 `forward()` 方法会根据输入数据计算模型的输出，并返回相应的结果。同时，模型会保留计算图以进行反向传播和梯度计算。\n",
    "\n",
    "在深度学习训练过程中，通常在训练阶段使用 `model.train()` 将模型设置为训练模式，以确保模型在训练期间按预期进行计算和参数更新。\n",
    "\n",
    "---\n",
    "\n",
    "**关于验证模式的参数**\n",
    "\n",
    "在验证模式下，我们不需要进行参数更新，因此将优化器（`Optimizer`）和学习率调度器（`Scheduler`）设置为虚拟（dummy）对象是一种常见做法。\n",
    "\n",
    "在验证模式中，我们只关心模型在验证集上的性能评估，而不需要通过优化器来更新模型的参数。因此，将优化器和学习率调度器设置为虚拟对象可以避免不必要的计算和参数更新。\n",
    "\n",
    "通过将优化器和学习率调度器设置为虚拟对象，我们可以在验证过程中仅关注模型的前向传播和损失计算，而无需进行反向传播和参数更新的计算，从而提高了验证过程的效率。这样可以减少显存的占用并加快验证的速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9169911c",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_distributed_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config):\n",
    "    ngpus = torch.cuda.device_count()\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12356\"\n",
    "    print(f\"Number of GPUs detected: {ngpus}\")\n",
    "    print(\"Spawning training processes ...\")\n",
    "    mp.spawn(\n",
    "        train_worker,\n",
    "        nprocs=ngpus,\n",
    "        args=(ngpus, vocab_src, vocab_tgt, spacy_de, spacy_en, config, True),\n",
    "    )\n",
    "\n",
    "\n",
    "def train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config):\n",
    "    if config[\"distributed\"]:\n",
    "        train_distributed_model(\n",
    "            vocab_src, vocab_tgt, spacy_de, spacy_en, config\n",
    "        )\n",
    "    else:\n",
    "        train_worker(\n",
    "            0, 1, vocab_src, vocab_tgt, spacy_de, spacy_en, config, False\n",
    "        )\n",
    "\n",
    "\n",
    "def load_trained_model():\n",
    "    config = {\n",
    "        \"batch_size\": 32,\n",
    "        \"distributed\": False,\n",
    "        \"num_epochs\": 8,\n",
    "        \"accum_iter\": 10,\n",
    "        \"base_lr\": 1.0,\n",
    "        \"max_padding\": 72,\n",
    "        \"warmup\": 3000,\n",
    "        \"file_prefix\": \"multi30k_model_\",\n",
    "    }\n",
    "    model_path = \"multi30k_model_final.pt\"\n",
    "    if not exists(model_path):\n",
    "        train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config)\n",
    "\n",
    "    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n",
    "    model.load_state_dict(torch.load(\"multi30k_model_final.pt\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "if is_interactive_notebook():\n",
    "    model = load_trained_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1630c6-a561-4b77-8a1c-f221aee45609",
   "metadata": {},
   "source": [
    "该代码片段涉及训练和加载已训练模型的过程。\n",
    "\n",
    "1. `train_distributed_model()`: 这是一个用于在分布式设置下进行训练的函数。它通过调用`mp.spawn()`来生成多个训练进程，每个进程在一个GPU上执行`train_worker()`函数。\n",
    "\n",
    "2. `train_model()`: 这是一个用于在单机设置下进行训练的函数。它直接调用`train_worker()`函数，将GPU设备编号设置为0，表示在单个GPU上进行训练。\n",
    "\n",
    "3. `load_trained_model()`: 这是一个加载已训练模型的函数。它首先检查是否存在已训练模型的文件。如果模型文件不存在，则调用`train_model()`函数进行训练。然后，它创建一个新的模型对象，使用`make_model()`函数构建模型结构，并从模型文件中加载训练好的参数。\n",
    "\n",
    "4. 在交互式笔记本环境中，调用`load_trained_model()`函数来加载已训练模型，并将其赋值给变量`model`。\n",
    "\n",
    "综上所述，这段代码实现了训练和加载模型的功能。它根据配置参数决定是进行分布式训练还是单机训练，并提供了加载已训练模型的选项。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270e3b18",
   "metadata": {
    "id": "RZK_VjDPTsqN"
   },
   "source": [
    "\n",
    "> Once trained we can decode the model to produce a set of\n",
    "> translations. Here we simply translate the first sentence in the\n",
    "> validation set. This dataset is pretty small so the translations\n",
    "> with greedy search are reasonably accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb79ced",
   "metadata": {
    "id": "L50i0iEXTsqN"
   },
   "source": [
    "# Additional Components: BPE, Search, Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c8692",
   "metadata": {
    "id": "NBx1C2_NTsqN"
   },
   "source": [
    "\n",
    "> So this mostly covers the transformer model itself. There are four\n",
    "> aspects that we didn't cover explicitly. We also have all these\n",
    "> additional features implemented in\n",
    "> [OpenNMT-py](https://github.com/opennmt/opennmt-py).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32485a0d",
   "metadata": {
    "id": "UpqV1mWnTsqN"
   },
   "source": [
    "\n",
    "> 1) BPE/ Word-piece: We can use a library to first preprocess the\n",
    "> data into subword units. See Rico Sennrich's\n",
    "> [subword-nmt](https://github.com/rsennrich/subword-nmt)\n",
    "> implementation. These models will transform the training data to\n",
    "> look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bec7da",
   "metadata": {
    "id": "hwJ_9J0BTsqN"
   },
   "source": [
    "▁Die ▁Protokoll datei ▁kann ▁ heimlich ▁per ▁E - Mail ▁oder ▁FTP\n",
    "▁an ▁einen ▁bestimmte n ▁Empfänger ▁gesendet ▁werden ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9306c49b",
   "metadata": {
    "id": "9HwejYkpTsqN"
   },
   "source": [
    "\n",
    "> 2) Shared Embeddings: When using BPE with shared vocabulary we can\n",
    "> share the same weight vectors between the source / target /\n",
    "> generator. See the [(cite)](https://arxiv.org/abs/1608.05859) for\n",
    "> details. To add this to the model simply do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ced196",
   "metadata": {
    "id": "tb3j3CYLTsqN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    model.src_embed[0].lut.weight = model.tgt_embeddings[0].lut.weight\n",
    "    model.generator.lut.weight = model.tgt_embed[0].lut.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9326a432",
   "metadata": {
    "id": "xDKJsSwRTsqN"
   },
   "source": [
    "\n",
    "> 3) Beam Search: This is a bit too complicated to cover here. See the\n",
    "> [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py/)\n",
    "> for a pytorch implementation.\n",
    ">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca5e97",
   "metadata": {
    "id": "wf3vVYGZTsqN"
   },
   "source": [
    "\n",
    "> 4) Model Averaging: The paper averages the last k checkpoints to\n",
    "> create an ensembling effect. We can do this after the fact if we\n",
    "> have a bunch of models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae98f3d8",
   "metadata": {
    "id": "hAFEa78JokDB",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def average(model, models):\n",
    "    \"Average models into model\"\n",
    "    for ps in zip(*[m.params() for m in [model] + models]):\n",
    "        ps[0].copy_(torch.sum(*ps[1:]) / len(ps[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a933402",
   "metadata": {
    "id": "Kz5BYJ9sTsqO"
   },
   "source": [
    "# Results\n",
    "\n",
    "On the WMT 2014 English-to-German translation task, the big\n",
    "transformer model (Transformer (big) in Table 2) outperforms the\n",
    "best previously reported models (including ensembles) by more than\n",
    "2.0 BLEU, establishing a new state-of-the-art BLEU score of\n",
    "28.4. The configuration of this model is listed in the bottom line\n",
    "of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base\n",
    "model surpasses all previously published models and ensembles, at a\n",
    "fraction of the training cost of any of the competitive models.\n",
    "\n",
    "On the WMT 2014 English-to-French translation task, our big model\n",
    "achieves a BLEU score of 41.0, outperforming all of the previously\n",
    "published single models, at less than 1/4 the training cost of the\n",
    "previous state-of-the-art model. The Transformer (big) model trained\n",
    "for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6928b60e",
   "metadata": {},
   "source": [
    "![](images/results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db991c",
   "metadata": {
    "id": "cPcnsHvQTsqO"
   },
   "source": [
    "\n",
    "\n",
    "> With the addtional extensions in the last section, the OpenNMT-py\n",
    "> replication gets to 26.9 on EN-DE WMT. Here I have loaded in those\n",
    "> parameters to our reimplemenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a5f64a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Load data and model for output checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afc05a8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def check_outputs(\n",
    "    valid_dataloader,\n",
    "    model,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    n_examples=15,\n",
    "    pad_idx=2,\n",
    "    eos_string=\"</s>\",\n",
    "):\n",
    "    results = [()] * n_examples\n",
    "    for idx in range(n_examples):\n",
    "        print(\"\\nExample %d ========\\n\" % idx)\n",
    "        b = next(iter(valid_dataloader))\n",
    "        rb = Batch(b[0], b[1], pad_idx)\n",
    "        greedy_decode(model, rb.src, rb.src_mask, 64, 0)[0]\n",
    "\n",
    "        src_tokens = [\n",
    "            vocab_src.get_itos()[x] for x in rb.src[0] if x != pad_idx\n",
    "        ]\n",
    "        tgt_tokens = [\n",
    "            vocab_tgt.get_itos()[x] for x in rb.tgt[0] if x != pad_idx\n",
    "        ]\n",
    "\n",
    "        print(\n",
    "            \"Source Text (Input)        : \"\n",
    "            + \" \".join(src_tokens).replace(\"\\n\", \"\")\n",
    "        )\n",
    "        print(\n",
    "            \"Target Text (Ground Truth) : \"\n",
    "            + \" \".join(tgt_tokens).replace(\"\\n\", \"\")\n",
    "        )\n",
    "        model_out = greedy_decode(model, rb.src, rb.src_mask, 72, 0)[0]\n",
    "        model_txt = (\n",
    "            \" \".join(\n",
    "                [vocab_tgt.get_itos()[x] for x in model_out if x != pad_idx]\n",
    "            ).split(eos_string, 1)[0]\n",
    "            + eos_string\n",
    "        )\n",
    "        print(\"Model Output               : \" + model_txt.replace(\"\\n\", \"\"))\n",
    "        results[idx] = (rb, src_tokens, tgt_tokens, model_out, model_txt)\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_model_example(n_examples=5):\n",
    "    global vocab_src, vocab_tgt, spacy_de, spacy_en\n",
    "\n",
    "    print(\"Preparing Data ...\")\n",
    "    _, valid_dataloader = create_dataloaders(\n",
    "        torch.device(\"cpu\"),\n",
    "        vocab_src,\n",
    "        vocab_tgt,\n",
    "        spacy_de,\n",
    "        spacy_en,\n",
    "        batch_size=1,\n",
    "        is_distributed=False,\n",
    "    )\n",
    "\n",
    "    print(\"Loading Trained Model ...\")\n",
    "\n",
    "    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n",
    "    model.load_state_dict(\n",
    "        torch.load(\"multi30k_model_final.pt\", map_location=torch.device(\"cpu\"))\n",
    "    )\n",
    "\n",
    "    print(\"Checking Model Outputs:\")\n",
    "    example_data = check_outputs(\n",
    "        valid_dataloader, model, vocab_src, vocab_tgt, n_examples=n_examples\n",
    "    )\n",
    "    return model, example_data\n",
    "\n",
    "\n",
    "# execute_example(run_model_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4ba663",
   "metadata": {
    "id": "0ZkkNTKLTsqO"
   },
   "source": [
    "## Attention Visualization\n",
    "\n",
    "> Even with a greedy decoder the translation looks pretty good. We\n",
    "> can further visualize it to see what is happening at each layer of\n",
    "> the attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e6008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtx2df(m, max_row, max_col, row_tokens, col_tokens):\n",
    "    \"convert a dense matrix to a data frame with row and column indices\"\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            (\n",
    "                r,\n",
    "                c,\n",
    "                float(m[r, c]),\n",
    "                \"%.3d %s\"\n",
    "                % (r, row_tokens[r] if len(row_tokens) > r else \"<blank>\"),\n",
    "                \"%.3d %s\"\n",
    "                % (c, col_tokens[c] if len(col_tokens) > c else \"<blank>\"),\n",
    "            )\n",
    "            for r in range(m.shape[0])\n",
    "            for c in range(m.shape[1])\n",
    "            if r < max_row and c < max_col\n",
    "        ],\n",
    "        # if float(m[r,c]) != 0 and r < max_row and c < max_col],\n",
    "        columns=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def attn_map(attn, layer, head, row_tokens, col_tokens, max_dim=30):\n",
    "    df = mtx2df(\n",
    "        attn[0, head].data,\n",
    "        max_dim,\n",
    "        max_dim,\n",
    "        row_tokens,\n",
    "        col_tokens,\n",
    "    )\n",
    "    return (\n",
    "        alt.Chart(data=df)\n",
    "        .mark_rect()\n",
    "        .encode(\n",
    "            x=alt.X(\"col_token\", axis=alt.Axis(title=\"\")),\n",
    "            y=alt.Y(\"row_token\", axis=alt.Axis(title=\"\")),\n",
    "            color=\"value\",\n",
    "            tooltip=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
    "        )\n",
    "        .properties(height=400, width=400)\n",
    "        .interactive()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df41498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_encoder(model, layer):\n",
    "    return model.encoder.layers[layer].self_attn.attn\n",
    "\n",
    "\n",
    "def get_decoder_self(model, layer):\n",
    "    return model.decoder.layers[layer].self_attn.attn\n",
    "\n",
    "\n",
    "def get_decoder_src(model, layer):\n",
    "    return model.decoder.layers[layer].src_attn.attn\n",
    "\n",
    "\n",
    "def visualize_layer(model, layer, getter_fn, ntokens, row_tokens, col_tokens):\n",
    "    # ntokens = last_example[0].ntokens\n",
    "    attn = getter_fn(model, layer)\n",
    "    n_heads = attn.shape[1]\n",
    "    charts = [\n",
    "        attn_map(\n",
    "            attn,\n",
    "            0,\n",
    "            h,\n",
    "            row_tokens=row_tokens,\n",
    "            col_tokens=col_tokens,\n",
    "            max_dim=ntokens,\n",
    "        )\n",
    "        for h in range(n_heads)\n",
    "    ]\n",
    "    assert n_heads == 8\n",
    "    return alt.vconcat(\n",
    "        charts[0]\n",
    "        # | charts[1]\n",
    "        | charts[2]\n",
    "        # | charts[3]\n",
    "        | charts[4]\n",
    "        # | charts[5]\n",
    "        | charts[6]\n",
    "        # | charts[7]\n",
    "        # layer + 1 due to 0-indexing\n",
    "    ).properties(title=\"Layer %d\" % (layer + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba55164",
   "metadata": {},
   "source": [
    "## Encoder Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4dce51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def viz_encoder_self():\n",
    "    model, example_data = run_model_example(n_examples=1)\n",
    "    example = example_data[\n",
    "        len(example_data) - 1\n",
    "    ]  # batch object for the final example\n",
    "\n",
    "    layer_viz = [\n",
    "        visualize_layer(\n",
    "            model, layer, get_encoder, len(example[1]), example[1], example[1]\n",
    "        )\n",
    "        for layer in range(6)\n",
    "    ]\n",
    "    return alt.hconcat(\n",
    "        layer_viz[0]\n",
    "        # & layer_viz[1]\n",
    "        & layer_viz[2]\n",
    "        # & layer_viz[3]\n",
    "        & layer_viz[4]\n",
    "        # & layer_viz[5]\n",
    "    )\n",
    "\n",
    "\n",
    "show_example(viz_encoder_self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03907ca",
   "metadata": {},
   "source": [
    "## Decoder Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efedfb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def viz_decoder_self():\n",
    "    model, example_data = run_model_example(n_examples=1)\n",
    "    example = example_data[len(example_data) - 1]\n",
    "\n",
    "    layer_viz = [\n",
    "        visualize_layer(\n",
    "            model,\n",
    "            layer,\n",
    "            get_decoder_self,\n",
    "            len(example[1]),\n",
    "            example[1],\n",
    "            example[1],\n",
    "        )\n",
    "        for layer in range(6)\n",
    "    ]\n",
    "    return alt.hconcat(\n",
    "        layer_viz[0]\n",
    "        & layer_viz[1]\n",
    "        & layer_viz[2]\n",
    "        & layer_viz[3]\n",
    "        & layer_viz[4]\n",
    "        & layer_viz[5]\n",
    "    )\n",
    "\n",
    "\n",
    "show_example(viz_decoder_self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e33697",
   "metadata": {},
   "source": [
    "## Decoder Src Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcae0f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def viz_decoder_src():\n",
    "    model, example_data = run_model_example(n_examples=1)\n",
    "    example = example_data[len(example_data) - 1]\n",
    "\n",
    "    layer_viz = [\n",
    "        visualize_layer(\n",
    "            model,\n",
    "            layer,\n",
    "            get_decoder_src,\n",
    "            max(len(example[1]), len(example[2])),\n",
    "            example[1],\n",
    "            example[2],\n",
    "        )\n",
    "        for layer in range(6)\n",
    "    ]\n",
    "    return alt.hconcat(\n",
    "        layer_viz[0]\n",
    "        & layer_viz[1]\n",
    "        & layer_viz[2]\n",
    "        & layer_viz[3]\n",
    "        & layer_viz[4]\n",
    "        & layer_viz[5]\n",
    "    )\n",
    "\n",
    "\n",
    "show_example(viz_decoder_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00904c05",
   "metadata": {
    "id": "nSseuCcATsqO"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    " Hopefully this code is useful for future research. Please reach\n",
    " out if you have any issues.\n",
    "\n",
    "\n",
    " Cheers,\n",
    " Sasha Rush, Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak,\n",
    " Stella Biderman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfef7cf-ff1a-498f-b313-a4c6dad1d7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
